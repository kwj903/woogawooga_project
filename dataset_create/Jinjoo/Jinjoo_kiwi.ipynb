{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0968f01",
   "metadata": {},
   "source": [
    "# 키위 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f719a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kiwipiepy\n",
      "  Downloading kiwipiepy-0.21.0-cp310-cp310-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.2/2.4 MB 3.7 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.5/2.4 MB 6.4 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.9/2.4 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.3/2.4 MB 7.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.6/2.4 MB 8.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.9/2.4 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 2.2/2.4 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.4/2.4 MB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kiwipiepy) (2.2.6)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.5/78.5 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting kiwipiepy_model<0.22,>=0.21\n",
      "  Downloading kiwipiepy_model-0.21.0.tar.gz (35.5 MB)\n",
      "     ---------------------------------------- 0.0/35.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.4/35.5 MB 8.2 MB/s eta 0:00:05\n",
      "      --------------------------------------- 0.8/35.5 MB 9.8 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 1.1/35.5 MB 9.0 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 1.4/35.5 MB 9.0 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 1.7/35.5 MB 8.3 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 2.0/35.5 MB 7.8 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 2.2/35.5 MB 7.3 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 2.5/35.5 MB 7.0 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 2.8/35.5 MB 7.1 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 3.0/35.5 MB 6.8 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 3.2/35.5 MB 6.9 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 3.5/35.5 MB 6.8 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 3.9/35.5 MB 7.0 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 4.1/35.5 MB 6.7 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 4.6/35.5 MB 6.9 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 4.8/35.5 MB 7.0 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 5.3/35.5 MB 7.0 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 5.6/35.5 MB 7.0 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 6.0/35.5 MB 7.0 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 6.4/35.5 MB 7.1 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 7.0/35.5 MB 7.3 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 7.4/35.5 MB 7.4 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 7.8/35.5 MB 7.3 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 8.3/35.5 MB 7.6 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 8.6/35.5 MB 7.6 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 9.1/35.5 MB 7.8 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 9.5/35.5 MB 7.7 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 9.9/35.5 MB 7.8 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 10.5/35.5 MB 7.9 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 10.9/35.5 MB 7.9 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 11.3/35.5 MB 8.0 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 11.8/35.5 MB 8.2 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 12.2/35.5 MB 8.3 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 12.5/35.5 MB 8.3 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 12.9/35.5 MB 8.5 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 13.4/35.5 MB 8.6 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 13.8/35.5 MB 9.0 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 14.3/35.5 MB 9.2 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 14.9/35.5 MB 9.2 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 15.5/35.5 MB 9.5 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 15.9/35.5 MB 9.8 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 16.5/35.5 MB 9.9 MB/s eta 0:00:02\n",
      "     ------------------ -------------------- 17.0/35.5 MB 10.1 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 17.4/35.5 MB 10.2 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 18.0/35.5 MB 10.2 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 18.6/35.5 MB 10.4 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 19.2/35.5 MB 10.6 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 19.6/35.5 MB 10.6 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 20.1/35.5 MB 10.6 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 20.6/35.5 MB 10.6 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 21.1/35.5 MB 10.9 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 21.5/35.5 MB 10.7 MB/s eta 0:00:02\n",
      "     ------------------------ -------------- 22.0/35.5 MB 10.9 MB/s eta 0:00:02\n",
      "     ------------------------ -------------- 22.5/35.5 MB 11.3 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 23.0/35.5 MB 11.3 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 23.4/35.5 MB 11.3 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 23.9/35.5 MB 11.5 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 24.4/35.5 MB 11.5 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 24.8/35.5 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 25.1/35.5 MB 11.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 25.5/35.5 MB 11.3 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 25.8/35.5 MB 11.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 26.1/35.5 MB 10.7 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 26.3/35.5 MB 10.4 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 26.5/35.5 MB 10.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 26.6/35.5 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 26.7/35.5 MB 9.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 27.0/35.5 MB 9.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 27.3/35.5 MB 9.5 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 27.5/35.5 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 27.8/35.5 MB 9.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 28.1/35.5 MB 9.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 28.4/35.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 28.7/35.5 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 29.1/35.5 MB 8.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 29.4/35.5 MB 8.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 29.7/35.5 MB 8.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 30.0/35.5 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 30.2/35.5 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 30.5/35.5 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 30.8/35.5 MB 8.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 31.1/35.5 MB 7.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 31.5/35.5 MB 7.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 31.8/35.5 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 32.1/35.5 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 32.4/35.5 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 32.8/35.5 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 33.3/35.5 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 33.6/35.5 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 34.0/35.5 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 34.2/35.5 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  34.6/35.5 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  35.0/35.5 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  35.5/35.5 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  35.5/35.5 MB 7.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  35.5/35.5 MB 7.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 35.5/35.5 MB 6.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->kiwipiepy) (0.4.6)\n",
      "Installing collected packages: kiwipiepy_model, tqdm, kiwipiepy\n",
      "  Running setup.py install for kiwipiepy_model: started\n",
      "  Running setup.py install for kiwipiepy_model: finished with status 'done'\n",
      "Successfully installed kiwipiepy-0.21.0 kiwipiepy_model-0.21.0 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: kiwipiepy_model is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kiwipiepy"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "a10b4aae",
   "metadata": {},
   "source": [
    "- 취합한 데이터가 화자분리가 된 상태라 같은 파일이어도 여러 행을 가지고 있음.  \n",
    "- 어간 추출, 표제어 추출, 빈도수 확인을 위해 파일 이름 기준 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "cell_type": "code",
   "execution_count": 5,
>>>>>>> ddf91fa048d35a4ac935fe2ebcebf5ee89bcb4f5
   "id": "4cb8eb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 2047개의 전체 대화가 준비되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>phishing_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phishing_001</td>\n",
       "      <td>아빠? 목소리가 좀 이상한데요. 무슨 일인데 그렇게 급해요? 아빠 핸드폰이 고장 나...</td>\n",
       "      <td>가족지인사칭형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phishing_002</td>\n",
       "      <td>무슨 일이야? 왜 돈이 필요해?  핸드폰을 잃어버려서 그런데, 급히 돈 좀 빌려 줄...</td>\n",
       "      <td>가족지인사칭형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phishing_003</td>\n",
       "      <td>정말요? 어떤 병원이에요?  여보세요, OOO? 나 엄마 친구야.  아, 네. 무슨...</td>\n",
       "      <td>가족지인사칭형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phishing_004</td>\n",
       "      <td>나 사고가 나서 목소리가 변했어. 지금 병원비가 필요해.  OOO야, 나야. 오랜만...</td>\n",
       "      <td>가족지인사칭형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phishing_005</td>\n",
       "      <td>네 아들이 교통사고를 당했어요. 병원비 처리를 위해 급히 송금이 필요합니다.  여보...</td>\n",
       "      <td>가족지인사칭형</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_name                                               text  \\\n",
       "0  phishing_001  아빠? 목소리가 좀 이상한데요. 무슨 일인데 그렇게 급해요? 아빠 핸드폰이 고장 나...   \n",
       "1  phishing_002  무슨 일이야? 왜 돈이 필요해?  핸드폰을 잃어버려서 그런데, 급히 돈 좀 빌려 줄...   \n",
       "2  phishing_003  정말요? 어떤 병원이에요?  여보세요, OOO? 나 엄마 친구야.  아, 네. 무슨...   \n",
       "3  phishing_004  나 사고가 나서 목소리가 변했어. 지금 병원비가 필요해.  OOO야, 나야. 오랜만...   \n",
       "4  phishing_005  네 아들이 교통사고를 당했어요. 병원비 처리를 위해 급히 송금이 필요합니다.  여보...   \n",
       "\n",
       "  phishing_type  \n",
       "0       가족지인사칭형  \n",
       "1       가족지인사칭형  \n",
       "2       가족지인사칭형  \n",
       "3       가족지인사칭형  \n",
       "4       가족지인사칭형  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "\n",
<<<<<<< HEAD
    "# 데이터 불러오기 및 전처리\n",
    "df = pd.read_csv(\"phishing_data.csv\")\n",
    "df.dropna(subset=[\"text\"], inplace=True)  # 빈 text 행 삭제\n",
    "\n",
    "# file_name 기준으로 그룹화하여 대화 단위 데이터 생성\n",
=======
    "# 2. 데이터 불러오기 및 전처리\n",
    "df = pd.read_csv(\"phishing_data.csv\")\n",
    "df.dropna(subset=[\"text\"], inplace=True)  # 빈 text 행 삭제\n",
    "\n",
    "# 3. file_name 기준으로 그룹화하여 대화 단위 데이터 생성\n",
>>>>>>> ddf91fa048d35a4ac935fe2ebcebf5ee89bcb4f5
    "grouped_df = (\n",
    "    df.groupby(\"file_name\")\n",
    "    .agg({\"text\": \" \".join, \"phishing_type\": \"first\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"총 {len(grouped_df)}개의 전체 대화가 준비되었습니다.\")\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d77b2f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< 함수 테스트 >>\n",
      "원본: 아빠? 목소리가 좀 이상한데요. 무슨 일인데 그렇게 급해요? 아빠 핸드폰이 고장 나서 친구 폰으로 걸었어. 급하게 돈이 필요해서. 교통사고가 나서 병원비가 필요해. 믿고 바로 보내...\n",
      "추출된 키워드: ['아빠', '목소리', '이상', '급하', '아빠', '핸드폰', '고장', '친구', '급하', '필요', '교통사고', '병원비', '필요', '보내', '아빠']\n"
     ]
    }
   ],
   "source": [
    "# 1. Kiwi 객체 생성 (초기화에 시간이 걸리므로 한 번만 생성)\n",
    "kiwi = Kiwi()\n",
    "\n",
    "\n",
    "# 2. 키워드 추출 함수 정의\n",
    "def extract_keywords(text):\n",
    "    # 형태소 분석 실행\n",
    "    tokens = kiwi.analyze(text)[0][0]\n",
    "\n",
    "    keywords = []\n",
    "    # 각 형태소를 순회하며 조건에 맞는 키워드만 추출\n",
    "    for token in tokens:\n",
    "        # (1) 명사(NNG, NNP), 동사(VV), 형용사(VA)만 선택\n",
    "        # (2) 한 글자 단어는 제외 (노이즈 제거)\n",
    "        if token.tag in [\"NNG\", \"NNP\", \"VV\", \"VA\"] and len(token.form) > 1:\n",
    "            # 표제어(기본형)를 리스트에 추가\n",
    "            keywords.append(token.form)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "\n",
    "# 함수 테스트\n",
    "sample_text = grouped_df[\"text\"][0]\n",
    "sample_keywords = extract_keywords(sample_text)\n",
    "print(\"<< 함수 테스트 >>\")\n",
    "print(f\"원본: {sample_text[:100]}...\")\n",
    "print(f\"추출된 키워드: {sample_keywords}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 7,
>>>>>>> ddf91fa048d35a4ac935fe2ebcebf5ee89bcb4f5
   "id": "dad9f130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\woogawooga\\woogawooga_project\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 2047/2047 [00:38<00:00, 52.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 대화에 대한 키워드 추출을 시작합니다. (시간이 소요될 수 있습니다)\n",
      "키워드 추출 완료.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아빠? 목소리가 좀 이상한데요. 무슨 일인데 그렇게 급해요? 아빠 핸드폰이 고장 나...</td>\n",
       "      <td>[아빠, 목소리, 이상, 급하, 아빠, 핸드폰, 고장, 친구, 급하, 필요, 교통사...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>무슨 일이야? 왜 돈이 필요해?  핸드폰을 잃어버려서 그런데, 급히 돈 좀 빌려 줄...</td>\n",
       "      <td>[필요, 핸드폰, 빌리, 가족, 다치, 병원비, 모자라, 부탁, 바꾸, 친구, 전화...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>정말요? 어떤 병원이에요?  여보세요, OOO? 나 엄마 친구야.  아, 네. 무슨...</td>\n",
       "      <td>[병원, 엄마, 친구, 엄마, 사고, 당하, 병원, 필요, 나중, 설명, 송금, 부탁]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>나 사고가 나서 목소리가 변했어. 지금 병원비가 필요해.  OOO야, 나야. 오랜만...</td>\n",
       "      <td>[사고, 목소리, 변하, 병원비, 필요, 오랜만, 나중, 설명, 보내, 사고, 괜찮...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>네 아들이 교통사고를 당했어요. 병원비 처리를 위해 급히 송금이 필요합니다.  여보...</td>\n",
       "      <td>[아들, 교통사고, 당하, 병원비, 처리, 위하, 송금, 필요, 경찰서, 정말, 병...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  아빠? 목소리가 좀 이상한데요. 무슨 일인데 그렇게 급해요? 아빠 핸드폰이 고장 나...   \n",
       "1  무슨 일이야? 왜 돈이 필요해?  핸드폰을 잃어버려서 그런데, 급히 돈 좀 빌려 줄...   \n",
       "2  정말요? 어떤 병원이에요?  여보세요, OOO? 나 엄마 친구야.  아, 네. 무슨...   \n",
       "3  나 사고가 나서 목소리가 변했어. 지금 병원비가 필요해.  OOO야, 나야. 오랜만...   \n",
       "4  네 아들이 교통사고를 당했어요. 병원비 처리를 위해 급히 송금이 필요합니다.  여보...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [아빠, 목소리, 이상, 급하, 아빠, 핸드폰, 고장, 친구, 급하, 필요, 교통사...  \n",
       "1  [필요, 핸드폰, 빌리, 가족, 다치, 병원비, 모자라, 부탁, 바꾸, 친구, 전화...  \n",
       "2   [병원, 엄마, 친구, 엄마, 사고, 당하, 병원, 필요, 나중, 설명, 송금, 부탁]  \n",
       "3  [사고, 목소리, 변하, 병원비, 필요, 오랜만, 나중, 설명, 보내, 사고, 괜찮...  \n",
       "4  [아들, 교통사고, 당하, 병원비, 처리, 위하, 송금, 필요, 경찰서, 정말, 병...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas의 apply 함수를 사용하여 모든 행에 키워드 추출 함수 적용\n",
<<<<<<< HEAD
    "# tqdm 라이브러리를 통해 진행상황 확인.\n",
=======
    "# tqdm 라이브러리를 설치하면 진행상황을 볼 수 있어 편리합니다. (pip install tqdm)\n",
>>>>>>> ddf91fa048d35a4ac935fe2ebcebf5ee89bcb4f5
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "grouped_df['keywords'] = grouped_df['text'].progress_apply(extract_keywords)\n",
    "\n",
    "print(\"전체 대화에 대한 키워드 추출을 시작합니다. (시간이 소요될 수 있습니다)\")\n",
    "grouped_df[\"keywords\"] = grouped_df[\"text\"].apply(extract_keywords)\n",
    "print(\"키워드 추출 완료.\")\n",
    "\n",
    "# 결과 확인\n",
    "grouped_df[[\"text\", \"keywords\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46fbc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 전체 키워드 빈도수 (상위 20개) ---\n",
      "[('본인', 5788), ('드리', 4076), ('고객', 3447), ('확인', 3394), ('통장', 2826), ('말씀', 2770), ('계좌', 2744), ('부분', 2145), ('은행', 2080), ('전화', 1991), ('연락', 1897), ('사건', 1858), ('금융', 1627), ('정보', 1580), ('진행', 1579), ('대하', 1558), ('사용', 1509), ('대출', 1411), ('조사', 1395), ('처리', 1344)]\n"
     ]
    }
   ],
   "source": [
    "# 1. 'keywords' 컬럼의 모든 리스트를 하나의 큰 리스트로 합치기\n",
    "all_keywords = [keyword for sublist in grouped_df[\"keywords\"] for keyword in sublist]\n",
    "\n",
    "# 2. Counter를 이용해 단어별 빈도수 계산\n",
    "word_counts = Counter(all_keywords)\n",
    "\n",
    "# 3. 가장 많이 등장한 상위 20개 키워드 출력\n",
    "print(\"--- 전체 키워드 빈도수 (상위 20개) ---\")\n",
    "print(word_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee720917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 상위 500개 키워드 데이터프레임 (상위 10개) ---\n",
      "  Keyword  Frequency\n",
      "0      본인       5788\n",
      "1      드리       4076\n",
      "2      고객       3447\n",
      "3      확인       3394\n",
      "4      통장       2826\n",
      "5      말씀       2770\n",
      "6      계좌       2744\n",
      "7      부분       2145\n",
      "8      은행       2080\n",
      "9      전화       1991\n",
      "\n",
      "'top_500_keywords.csv' 파일 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 1. 빈도수 상위 500개 키워드 추출\n",
    "top_500_keywords = word_counts.most_common(500)\n",
    "\n",
    "# 2. 결과를 보기 좋은 데이터프레임으로 변환\n",
    "top_500_df = pd.DataFrame(top_500_keywords, columns=[\"Keyword\", \"Frequency\"])\n",
    "\n",
    "# 3. CSV 파일로 저장\n",
    "top_500_df.to_csv(\"top_500_keywords.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n--- 상위 500개 키워드 데이터프레임 (상위 10개) ---\")\n",
    "print(top_500_df.head(10))\n",
    "\n",
    "print(\"\\n'top_500_keywords.csv' 파일 저장이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e5376",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### 0kt, komoran, kkma, kiwi 비교를 위해 키워드 추출 및 빈도수 확인. \n",
    "- 통일하지 않고 임의로 데이터 뽑아보기.(한개미만은 삭제했더니 유의미한 데이터도 삭제됨.)"
=======
    "# 비교용 키위 키워드 뽑기"
>>>>>>> ddf91fa048d35a4ac935fe2ebcebf5ee89bcb4f5
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11144253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 2047개의 전체 대화가 준비되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터 불러오기 및 전처리\n",
    "df = pd.read_csv(\"phishing_data.csv\")\n",
    "df.dropna(subset=[\"text\"], inplace=True)  # 빈 text 행 삭제\n",
    "\n",
    "# file_name 기준으로 그룹화하여 대화 단위 데이터 생성\n",
    "grouped_df = (\n",
    "    df.groupby(\"file_name\")\n",
    "    .agg({\"text\": \" \".join, \"phishing_type\": \"first\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"총 {len(grouped_df)}개의 전체 대화가 준비되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b76015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< 함수 테스트 >>\n",
      "추출된 키워드: ['계좌', '이상', '흐름', '발견']\n"
     ]
    }
   ],
   "source": [
    "# Kiwi 객체 생성\n",
    "kiwi = Kiwi()\n",
    "\n",
    "\n",
    "# 새로운 조건이 반영된 키워드 추출 함수 정의\n",
    "def extract_keywords_refined(text):\n",
    "    # 형태소 분석 실행\n",
    "    tokens = kiwi.analyze(text)[0][0]\n",
    "\n",
    "    keywords = []\n",
    "    # 각 형태소를 순회하며 조건에 맞는 키워드만 추출\n",
    "    for token in tokens:\n",
    "        # 조건 1: 품사는 'NNG', 'NNP', 'VV', 'VA' 중 하나\n",
    "        # 조건 2: 단어 길이는 한 글자 초과\n",
    "        # 조건 3: 단어 형태는 'OOO'가 아님\n",
    "        if (\n",
    "            token.tag in [\"NNG\", \"NNP\", \"VV\", \"VA\"]\n",
    "            and len(token.form) > 1\n",
    "            and token.form != \"OOO\"\n",
    "        ):\n",
    "            # 표제어(기본형)를 리스트에 추가\n",
    "            keywords.append(token.form)\n",
    "\n",
    "    # 조건 4: 만약 추출된 키워드 리스트가 비어있으면 '없음' 문자열 반환\n",
    "    if not keywords:\n",
    "        return \"없음\"\n",
    "    else:\n",
    "        return keywords\n",
    "\n",
    "\n",
    "# 함수 테스트\n",
    "sample_text_with_ooo = \"OOO님 계좌에서 이상한 흐름이 발견되었습니다.\"\n",
    "print(\"<< 함수 테스트 >>\")\n",
    "print(f\"추출된 키워드: {extract_keywords_refined(sample_text_with_ooo)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f253aa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 대화에 대한 키워드 추출을 시작합니다. (시간이 소요될 수 있습니다)\n",
      "키워드 추출 완료.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아빠? 목소리가 좀 이상한데요. 무슨 일인데 그렇게 급해요? 아빠 핸드폰이 고장 나...</td>\n",
       "      <td>[아빠, 목소리, 이상, 급하, 아빠, 핸드폰, 고장, 친구, 급하, 필요, 교통사...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>무슨 일이야? 왜 돈이 필요해?  핸드폰을 잃어버려서 그런데, 급히 돈 좀 빌려 줄...</td>\n",
       "      <td>[필요, 핸드폰, 빌리, 가족, 다치, 병원비, 모자라, 부탁, 바꾸, 친구, 전화...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>정말요? 어떤 병원이에요?  여보세요, OOO? 나 엄마 친구야.  아, 네. 무슨...</td>\n",
       "      <td>[병원, 엄마, 친구, 엄마, 사고, 당하, 병원, 필요, 나중, 설명, 송금, 부탁]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>나 사고가 나서 목소리가 변했어. 지금 병원비가 필요해.  OOO야, 나야. 오랜만...</td>\n",
       "      <td>[사고, 목소리, 변하, 병원비, 필요, 오랜만, 나중, 설명, 보내, 사고, 괜찮...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>네 아들이 교통사고를 당했어요. 병원비 처리를 위해 급히 송금이 필요합니다.  여보...</td>\n",
       "      <td>[아들, 교통사고, 당하, 병원비, 처리, 위하, 송금, 필요, 경찰서, 정말, 병...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  아빠? 목소리가 좀 이상한데요. 무슨 일인데 그렇게 급해요? 아빠 핸드폰이 고장 나...   \n",
       "1  무슨 일이야? 왜 돈이 필요해?  핸드폰을 잃어버려서 그런데, 급히 돈 좀 빌려 줄...   \n",
       "2  정말요? 어떤 병원이에요?  여보세요, OOO? 나 엄마 친구야.  아, 네. 무슨...   \n",
       "3  나 사고가 나서 목소리가 변했어. 지금 병원비가 필요해.  OOO야, 나야. 오랜만...   \n",
       "4  네 아들이 교통사고를 당했어요. 병원비 처리를 위해 급히 송금이 필요합니다.  여보...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [아빠, 목소리, 이상, 급하, 아빠, 핸드폰, 고장, 친구, 급하, 필요, 교통사...  \n",
       "1  [필요, 핸드폰, 빌리, 가족, 다치, 병원비, 모자라, 부탁, 바꾸, 친구, 전화...  \n",
       "2   [병원, 엄마, 친구, 엄마, 사고, 당하, 병원, 필요, 나중, 설명, 송금, 부탁]  \n",
       "3  [사고, 목소리, 변하, 병원비, 필요, 오랜만, 나중, 설명, 보내, 사고, 괜찮...  \n",
       "4  [아들, 교통사고, 당하, 병원비, 처리, 위하, 송금, 필요, 경찰서, 정말, 병...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas의 apply 함수를 사용하여 모든 행에 키워드 추출 함수 적용\n",
    "print(\"전체 대화에 대한 키워드 추출을 시작합니다. (시간이 소요될 수 있습니다)\")\n",
    "grouped_df[\"keywords\"] = grouped_df[\"text\"].apply(extract_keywords_refined)\n",
    "print(\"키워드 추출 완료.\")\n",
    "\n",
    "# 결과 확인 (일부 행에 '없음'이 표시될 수 있습니다)\n",
    "grouped_df[[\"text\", \"keywords\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0909117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 유효 키워드 개수: 188026\n",
      "\n",
      "--- 전체 키워드 빈도수 (상위 20개) ---\n",
      "[('본인', 5788), ('드리', 4076), ('고객', 3447), ('확인', 3394), ('통장', 2826), ('말씀', 2770), ('계좌', 2744), ('부분', 2145), ('은행', 2080), ('전화', 1991), ('연락', 1897), ('사건', 1858), ('금융', 1627), ('정보', 1580), ('진행', 1579), ('대하', 1558), ('사용', 1509), ('대출', 1411), ('조사', 1395), ('처리', 1344)]\n"
     ]
    }
   ],
   "source": [
    "# 'keywords' 컬럼에서 '없음'을 제외하고 모든 키워드를 하나의 리스트로 합치기\n",
    "all_keywords = [\n",
    "    keyword\n",
    "    for sublist in grouped_df[\"keywords\"]\n",
    "    if isinstance(sublist, list)  # sublist가 리스트인 경우에만 순회\n",
    "    for keyword in sublist\n",
    "]\n",
    "\n",
    "# 전체 키워드의 총개수 계산 (비율 계산에 사용)\n",
    "total_keyword_count = len(all_keywords)\n",
    "print(f\"전체 유효 키워드 개수: {total_keyword_count}\")\n",
    "\n",
    "# Counter를 이용해 단어별 빈도수 계산\n",
    "word_counts = Counter(all_keywords)\n",
    "\n",
    "# 가장 많이 등장한 상위 20개 키워드 출력\n",
    "print(\"\\n--- 전체 키워드 빈도수 (상위 20개) ---\")\n",
    "print(word_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "244074f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 상위 700개 키워드 (상위 10개, 총 개수, 비율) ---\n",
      "  Keyword  Frequency  Proportion(%)\n",
      "0      본인       5788          3.078\n",
      "1      드리       4076          2.168\n",
      "2      고객       3447          1.833\n",
      "3      확인       3394          1.805\n",
      "4      통장       2826          1.503\n",
      "5      말씀       2770          1.473\n",
      "6      계좌       2744          1.459\n",
      "7      부분       2145          1.141\n",
      "8      은행       2080          1.106\n",
      "9      전화       1991          1.059\n",
      "\n",
      "'top_700_keywords_with_ratio.csv' 파일 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 빈도수 상위 700개 키워드 추출\n",
    "top_700_keywords = word_counts.most_common(700)\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "top_700_df = pd.DataFrame(top_700_keywords, columns=[\"Keyword\", \"Frequency\"])\n",
    "\n",
    "# 비율(%) 컬럼 추가\n",
    "# (개별 키워드 빈도수 / 전체 유효 키워드 총개수) * 100\n",
    "top_700_df[\"Proportion(%)\"] = (\n",
    "    top_700_df[\"Frequency\"] / total_keyword_count * 100\n",
    ").round(3)\n",
    "\n",
    "\n",
    "# CSV 파일로 저장\n",
    "file_name_to_save = \"top_700_keywords_with_ratio.csv\"\n",
    "top_700_df.to_csv(file_name_to_save, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n--- 상위 700개 키워드 (상위 10개, 총 개수, 비율) ---\")\n",
    "print(top_700_df.head(10))\n",
    "\n",
    "print(f\"\\n'{file_name_to_save}' 파일 저장이 완료되었습니다.\")"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "6c03056f",
   "metadata": {},
   "source": [
    "### 일반대화 데이터 형태소 분리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "cell_type": "code",
   "execution_count": 17,
>>>>>>> ddf91fa048d35a4ac935fe2ebcebf5ee89bcb4f5
   "id": "faed95b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 11019개의 정상 대화가 준비되었습니다.\n"
     ]
    }
   ],
   "source": [
    "df_normal = pd.read_csv(\"normal_data.csv\")\n",
    "df_normal.dropna(subset=[\"text\"], inplace=True)  # 빈 text 행 삭제\n",
    "\n",
    "# file_name 기준으로 그룹화하여 대화 단위 데이터 생성\n",
    "# 이전 데이터와 구분하기 위해 변수명에 _normal을 붙입니다.\n",
    "grouped_df_normal = (\n",
    "    df_normal.groupby(\"file_name\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"text\": \" \".join,\n",
<<<<<<< HEAD
=======
    "            # 'phishing_type' 컬럼이 있다면 포함시키고, 없다면 이 줄은 삭제하세요.\n",
    "            # 'phishing_type': 'first'\n",
>>>>>>> ddf91fa048d35a4ac935fe2ebcebf5ee89bcb4f5
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"총 {len(grouped_df_normal)}개의 정상 대화가 준비되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc953bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드 추출 함수가 준비되었습니다.\n"
     ]
    }
   ],
   "source": [
    "kiwi = Kiwi()\n",
    "\n",
    "\n",
    "# 새로운 조건이 반영된 키워드 추출 함수 정의\n",
    "def extract_keywords_refined(text):\n",
    "    tokens = kiwi.analyze(text)[0][0]\n",
    "    keywords = []\n",
    "    for token in tokens:\n",
    "        if (\n",
    "            token.tag in [\"NNG\", \"NNP\", \"VV\", \"VA\"]\n",
    "            and len(token.form) > 1\n",
    "            and token.form != \"OOO\", \"ㅇㅇㅇ\"\n",
    "        ):\n",
    "            keywords.append(token.form)\n",
    "\n",
    "    if not keywords:\n",
    "        return \"없음\"\n",
    "    else:\n",
    "        return keywords\n",
    "\n",
    "\n",
    "print(\"키워드 추출 함수가 준비되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3fbbe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 정상 대화에 대한 키워드 추출을 시작합니다. (시간이 소요될 수 있습니다)\n",
      "키워드 추출 완료.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>여보세요 여보세요 여기요 안녕하세요 네 안녕하세요 오늘은 패스트푸드에 피자에 대해서...</td>\n",
       "      <td>[안녕, 안녕, 오늘, 패스트푸드, 피자, 대하, 얘기, 피자요, 피자, 좋아하, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>에 안녕하세요 얼마 아 네 안녕하세요 오늘은 무척 늦었어요 소리가 잘 들려서요 어우...</td>\n",
       "      <td>[안녕, 얼마, 안녕, 오늘, 소리, 들리, 들리, 다행, 오늘, 가지, 대화, 오...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>반갑습니다 미래에 반갑습니다 잘 지내셨죠 네 잘 지내고 있어요 더운데 요즘은 뭐를 ...</td>\n",
       "      <td>[미래, 지내, 지내, 요즘, 드시, 오이, 냉국, 쇼핑, 김치, 젓갈, 식품, 홈...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>여보세 아 여보세요? 아 잘 들려?  어 안녕하세요? 어 안녕하세요, 잘 들립니까?...</td>\n",
       "      <td>[들리, 안녕, 안녕하세요, 들리, 주제, 탕류, 설렁탕, 갈비탕, 소고기, 대화,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>아 안녕하세요 안녕하세요 통화가 잘 안됐었는데 잘 부탁드립니다아 그럼 잘 부탁드립니...</td>\n",
       "      <td>[안녕하세요, 안녕하세요, 통화, 부탁, 드리, 부탁, 드리, 민아, 부탁, 할인,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  여보세요 여보세요 여기요 안녕하세요 네 안녕하세요 오늘은 패스트푸드에 피자에 대해서...   \n",
       "1  에 안녕하세요 얼마 아 네 안녕하세요 오늘은 무척 늦었어요 소리가 잘 들려서요 어우...   \n",
       "2  반갑습니다 미래에 반갑습니다 잘 지내셨죠 네 잘 지내고 있어요 더운데 요즘은 뭐를 ...   \n",
       "3  여보세 아 여보세요? 아 잘 들려?  어 안녕하세요? 어 안녕하세요, 잘 들립니까?...   \n",
       "4  아 안녕하세요 안녕하세요 통화가 잘 안됐었는데 잘 부탁드립니다아 그럼 잘 부탁드립니...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [안녕, 안녕, 오늘, 패스트푸드, 피자, 대하, 얘기, 피자요, 피자, 좋아하, ...  \n",
       "1  [안녕, 얼마, 안녕, 오늘, 소리, 들리, 들리, 다행, 오늘, 가지, 대화, 오...  \n",
       "2  [미래, 지내, 지내, 요즘, 드시, 오이, 냉국, 쇼핑, 김치, 젓갈, 식품, 홈...  \n",
       "3  [들리, 안녕, 안녕하세요, 들리, 주제, 탕류, 설렁탕, 갈비탕, 소고기, 대화,...  \n",
       "4  [안녕하세요, 안녕하세요, 통화, 부탁, 드리, 부탁, 드리, 민아, 부탁, 할인,...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply 함수를 사용하여 모든 행에 키워드 추출 함수 적용\n",
    "print(\"전체 정상 대화에 대한 키워드 추출을 시작합니다. (시간이 소요될 수 있습니다)\")\n",
    "grouped_df_normal[\"keywords\"] = grouped_df_normal[\"text\"].apply(\n",
    "    extract_keywords_refined\n",
    ")\n",
    "print(\"키워드 추출 완료.\")\n",
    "\n",
    "# 결과 확인\n",
    "grouped_df_normal[[\"text\", \"keywords\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb86f4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 유효 키워드 개수 (정상 대화): 985562\n",
      "\n",
      "--- 정상 대화 키워드 빈도수 (상위 20개) ---\n",
      "[('확인', 28242), ('교재', 18717), ('감사', 18297), ('환불', 17476), ('드리', 17093), ('회원', 16150), ('말씀', 12987), ('결제', 12084), ('취소', 10862), ('그러', 9769), ('가능', 9693), ('구매', 7682), ('전화', 7346), ('기다리', 7324), ('생각', 7278), ('보내', 7004), ('진행', 6681), ('연락', 6324), ('사람', 6038), ('카드', 5667)]\n"
     ]
    }
   ],
   "source": [
    "# 1. 'keywords' 컬럼에서 '없음'을 제외하고 모든 키워드를 하나의 리스트로 합치기\n",
    "all_keywords_normal = [\n",
    "    keyword\n",
    "    for sublist in grouped_df_normal[\"keywords\"]\n",
    "    if isinstance(sublist, list)\n",
    "    for keyword in sublist\n",
    "]\n",
    "\n",
    "# 2. 전체 키워드의 총개수 계산\n",
    "total_keyword_count_normal = len(all_keywords_normal)\n",
    "print(f\"전체 유효 키워드 개수 (정상 대화): {total_keyword_count_normal}\")\n",
    "\n",
    "# 3. Counter를 이용해 단어별 빈도수 계산\n",
    "word_counts_normal = Counter(all_keywords_normal)\n",
    "\n",
    "# 4. 가장 많이 등장한 상위 20개 키워드 출력\n",
    "print(\"\\n--- 정상 대화 키워드 빈도수 (상위 20개) ---\")\n",
    "print(word_counts_normal.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a010778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 정상 대화 상위 700개 키워드 (상위 10개, 총 개수, 비율) ---\n",
      "  Keyword  Frequency  Proportion(%)\n",
      "0      확인      28242          2.866\n",
      "1      교재      18717          1.899\n",
      "2      감사      18297          1.857\n",
      "3      환불      17476          1.773\n",
      "4      드리      17093          1.734\n",
      "5      회원      16150          1.639\n",
      "6      말씀      12987          1.318\n",
      "7      결제      12084          1.226\n",
      "8      취소      10862          1.102\n",
      "9      그러       9769          0.991\n",
      "\n",
      "'top_700_keywords_normal_with_ratio.csv' 파일 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 1. 빈도수 상위 700개 키워드 추출\n",
    "top_700_keywords_normal = word_counts_normal.most_common(700)\n",
    "\n",
    "# 2. 결과를 데이터프레임으로 변환\n",
    "top_700_df_normal = pd.DataFrame(\n",
    "    top_700_keywords_normal, columns=[\"Keyword\", \"Frequency\"]\n",
    ")\n",
    "\n",
    "# 3. 비율(%) 컬럼 추가\n",
    "top_700_df_normal[\"Proportion(%)\"] = (\n",
    "    top_700_df_normal[\"Frequency\"] / total_keyword_count_normal * 100\n",
    ").round(3)\n",
    "\n",
    "# 4. CSV 파일로 저장 (파일 이름 변경)\n",
    "file_name_to_save = \"top_700_keywords_normal_with_ratio.csv\"\n",
    "top_700_df_normal.to_csv(file_name_to_save, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n--- 정상 대화 상위 700개 키워드 (상위 10개, 총 개수, 비율) ---\")\n",
    "print(top_700_df_normal.head(10))\n",
    "\n",
    "print(f\"\\n'{file_name_to_save}' 파일 저장이 완료되었습니다.\")"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "5f62f7db",
   "metadata": {},
   "source": [
    "### 처음 글자수 제한을 둬 나오지 않던 단어들까지 포함되도록 재진행"
   ]
  },
  {
=======
>>>>>>> ddf91fa048d35a4ac935fe2ebcebf5ee89bcb4f5
   "cell_type": "code",
   "execution_count": 22,
   "id": "c59a48b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 2047개의 전체 대화가 준비되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터 불러오기 및 전처리\n",
    "df = pd.read_csv(\"phishing_data.csv\")\n",
    "df.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "# file_name 기준으로 그룹화\n",
    "grouped_df = (\n",
    "    df.groupby(\"file_name\")\n",
    "    .agg({\"text\": \" \".join, \"phishing_type\": \"first\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"총 {len(grouped_df)}개의 전체 대화가 준비되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42ac41e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< 함수 테스트 >>\n",
      "추출된 키워드: ['확인', '가능', '연락', '드리']\n"
     ]
    }
   ],
   "source": [
    "# Kiwi 객체 생성\n",
    "kiwi = Kiwi()\n",
    "\n",
    "\n",
    "# 불용어 처리가 제외된 키워드 추출 함수 정의\n",
    "def extract_keywords_no_stopwords(text):\n",
    "    tokens = kiwi.analyze(text)[0][0]\n",
    "    keywords = []\n",
    "    for token in tokens:\n",
    "        # 조건 1: 품사는 'NNG', 'NNP', 'VV', 'VA' 중 하나\n",
    "        # 조건 2: 단어 형태는 'OOO'가 아님\n",
    "        if token.tag in [\"NNG\", \"NNP\", \"VV\", \"VA\"] and token.form != \"OOO\":\n",
    "            # 표제어(기본형)를 리스트에 추가\n",
    "            keywords.append(token.form)\n",
    "\n",
    "    # 추출된 키워드가 없으면 '없음' 반환\n",
    "    if not keywords:\n",
    "        return \"없음\"\n",
    "    else:\n",
    "        return keywords\n",
    "\n",
    "\n",
    "# 함수 테스트\n",
    "sample_text = \"지금 바로 확인 가능한가요? 제가 OOO씨한테 연락 드릴게요.\"\n",
    "print(\"<< 함수 테스트 >>\")\n",
    "print(f\"추출된 키워드: {extract_keywords_no_stopwords(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8981928d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 처리 없이 키워드 추출을 시작합니다...\n",
      "키워드 추출 완료.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아빠? 목소리가 좀 이상한데요. 무슨 일인데 그렇게 급해요? 아빠 핸드폰이 고장 나...</td>\n",
       "      <td>[아빠, 목소리, 이상, 일, 급하, 아빠, 핸드폰, 고장, 나, 친구, 폰, 급하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>무슨 일이야? 왜 돈이 필요해?  핸드폰을 잃어버려서 그런데, 급히 돈 좀 빌려 줄...</td>\n",
       "      <td>[일, 돈, 필요, 핸드폰, 잃, 돈, 빌리, 있, 가족, 다치, 병원비, 모자라,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>정말요? 어떤 병원이에요?  여보세요, OOO? 나 엄마 친구야.  아, 네. 무슨...</td>\n",
       "      <td>[병원, 엄마, 친구, 일, 엄마, 사고, 당하, 병원, 있, 돈, 필요, 나중, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>나 사고가 나서 목소리가 변했어. 지금 병원비가 필요해.  OOO야, 나야. 오랜만...</td>\n",
       "      <td>[나, 사고, 나, 목소리, 변하, 병원비, 필요, 오랜만, 보, 되, 나중, 설명...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>네 아들이 교통사고를 당했어요. 병원비 처리를 위해 급히 송금이 필요합니다.  여보...</td>\n",
       "      <td>[아들, 교통사고, 당하, 병원비, 처리, 위하, 송금, 필요, 경찰서, 정말, 병...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  아빠? 목소리가 좀 이상한데요. 무슨 일인데 그렇게 급해요? 아빠 핸드폰이 고장 나...   \n",
       "1  무슨 일이야? 왜 돈이 필요해?  핸드폰을 잃어버려서 그런데, 급히 돈 좀 빌려 줄...   \n",
       "2  정말요? 어떤 병원이에요?  여보세요, OOO? 나 엄마 친구야.  아, 네. 무슨...   \n",
       "3  나 사고가 나서 목소리가 변했어. 지금 병원비가 필요해.  OOO야, 나야. 오랜만...   \n",
       "4  네 아들이 교통사고를 당했어요. 병원비 처리를 위해 급히 송금이 필요합니다.  여보...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [아빠, 목소리, 이상, 일, 급하, 아빠, 핸드폰, 고장, 나, 친구, 폰, 급하...  \n",
       "1  [일, 돈, 필요, 핸드폰, 잃, 돈, 빌리, 있, 가족, 다치, 병원비, 모자라,...  \n",
       "2  [병원, 엄마, 친구, 일, 엄마, 사고, 당하, 병원, 있, 돈, 필요, 나중, ...  \n",
       "3  [나, 사고, 나, 목소리, 변하, 병원비, 필요, 오랜만, 보, 되, 나중, 설명...  \n",
       "4  [아들, 교통사고, 당하, 병원비, 처리, 위하, 송금, 필요, 경찰서, 정말, 병...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"불용어 처리 없이 키워드 추출을 시작합니다...\")\n",
    "grouped_df[\"keywords\"] = grouped_df[\"text\"].apply(extract_keywords_no_stopwords)\n",
    "print(\"키워드 추출 완료.\")\n",
    "grouped_df[[\"text\", \"keywords\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e7f11c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 유효 키워드 개수: 254774\n",
      "\n",
      "--- 전체 키워드 빈도수 (상위 20개) ---\n",
      "[('하', 11258), ('되', 9559), ('있', 6660), ('본인', 5788), ('드리', 4076), ('없', 3569), ('고객', 3447), ('확인', 3394), ('통장', 2826), ('말씀', 2770), ('계좌', 2744), ('알', 2452), ('부분', 2145), ('은행', 2080), ('전화', 1991), ('연락', 1897), ('사건', 1858), ('금융', 1627), ('같', 1617), ('정보', 1580)]\n"
     ]
    }
   ],
   "source": [
    "# '없음'을 제외하고 모든 키워드를 하나의 리스트로 합치기\n",
    "all_keywords = [\n",
    "    keyword\n",
    "    for sublist in grouped_df[\"keywords\"]\n",
    "    if isinstance(sublist, list)\n",
    "    for keyword in sublist\n",
    "]\n",
    "\n",
    "# 전체 키워드 총개수 계산\n",
    "total_keyword_count = len(all_keywords)\n",
    "print(f\"전체 유효 키워드 개수: {total_keyword_count}\")\n",
    "\n",
    "# 단어별 빈도수 계산\n",
    "word_counts = Counter(all_keywords)\n",
    "\n",
    "# 상위 20개 키워드 출력\n",
    "print(\"\\n--- 전체 키워드 빈도수 (상위 20개) ---\")\n",
    "print(word_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "168b117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 상위 700개 키워드 (상위 10개) ---\n",
      "  Keyword  Frequency  Proportion(%)\n",
      "0       하      11258          4.419\n",
      "1       되       9559          3.752\n",
      "2       있       6660          2.614\n",
      "3      본인       5788          2.272\n",
      "4      드리       4076          1.600\n",
      "5       없       3569          1.401\n",
      "6      고객       3447          1.353\n",
      "7      확인       3394          1.332\n",
      "8      통장       2826          1.109\n",
      "9      말씀       2770          1.087\n",
      "\n",
      "'top_700_keywords_no_stopwords.csv' 파일 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 빈도수 상위 700개 키워드 추출\n",
    "top_700_keywords = word_counts.most_common(700)\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "top_700_df = pd.DataFrame(top_700_keywords, columns=[\"Keyword\", \"Frequency\"])\n",
    "\n",
    "# 비율(%) 컬럼 추가\n",
    "top_700_df[\"Proportion(%)\"] = (\n",
    "    top_700_df[\"Frequency\"] / total_keyword_count * 100\n",
    ").round(3)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "file_name_to_save = \"top_700_keywords_no_stopwords.csv\"\n",
    "top_700_df.to_csv(file_name_to_save, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n--- 상위 700개 키워드 (상위 10개) ---\")\n",
    "print(top_700_df.head(10))\n",
    "\n",
    "print(f\"\\n'{file_name_to_save}' 파일 저장이 완료되었습니다.\")"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "6cab5bdc",
   "metadata": {},
   "source": [
    "### 일반대화 데이터도 글자수 제한없애 1글자도 포함되도록 재진행"
   ]
  },
  {
=======
>>>>>>> ddf91fa048d35a4ac935fe2ebcebf5ee89bcb4f5
   "cell_type": "code",
   "execution_count": 28,
   "id": "454fe2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 11019개의 정상 대화가 준비되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# normal_data.csv 파일 불러오기 및 전처리\n",
    "df_normal = pd.read_csv(\"normal_data.csv\")\n",
    "df_normal.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "# file_name 기준으로 그룹화 (변수명에 _normal 추가)\n",
    "grouped_df_normal = (\n",
    "    df_normal.groupby(\"file_name\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"text\": \" \".join,\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"총 {len(grouped_df_normal)}개의 정상 대화가 준비되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7345cb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드 추출 함수가 준비되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Kiwi 객체 생성\n",
    "kiwi = Kiwi()\n",
    "\n",
    "\n",
    "# 불용어 처리가 제외된 키워드 추출 함수\n",
    "def extract_keywords_no_stopwords(text):\n",
    "    tokens = kiwi.analyze(text)[0][0]\n",
    "    keywords = []\n",
    "    for token in tokens:\n",
    "        if token.tag in [\"NNG\", \"NNP\", \"VV\", \"VA\"] and token.form != \"OOO\":\n",
    "            keywords.append(token.form)\n",
    "\n",
    "    if not keywords:\n",
    "        return \"없음\"\n",
    "    else:\n",
    "        return keywords\n",
    "\n",
    "\n",
    "print(\"키워드 추출 함수가 준비되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88742198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정상 대화에 대한 키워드 추출을 시작합니다...\n",
      "키워드 추출 완료.\n"
     ]
    }
   ],
   "source": [
    "print(\"정상 대화에 대한 키워드 추출을 시작합니다...\")\n",
    "grouped_df_normal[\"keywords\"] = grouped_df_normal[\"text\"].apply(\n",
    "    extract_keywords_no_stopwords\n",
    ")\n",
    "print(\"키워드 추출 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9447faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 유효 키워드 개수 (정상 대화): 1448894\n",
      "\n",
      "--- 정상 대화 키워드 빈도수 (상위 20개) ---\n",
      "[('하', 71818), ('되', 60325), ('있', 37434), ('확인', 28242), ('같', 20316), ('교재', 18717), ('감사', 18297), ('환불', 17476), ('드리', 17093), ('회원', 16150), ('알', 14709), ('말씀', 12987), ('결제', 12084), ('맞', 11957), ('없', 11956), ('취소', 10862), ('보', 10800), ('때', 10681), ('가', 10463), ('좋', 10093)]\n"
     ]
    }
   ],
   "source": [
    "# '없음'을 제외하고 모든 키워드를 하나의 리스트로 합치기\n",
    "all_keywords_normal = [\n",
    "    keyword\n",
    "    for sublist in grouped_df_normal[\"keywords\"]\n",
    "    if isinstance(sublist, list)\n",
    "    for keyword in sublist\n",
    "]\n",
    "\n",
    "# 전체 키워드 총개수 계산\n",
    "total_keyword_count_normal = len(all_keywords_normal)\n",
    "print(f\"전체 유효 키워드 개수 (정상 대화): {total_keyword_count_normal}\")\n",
    "\n",
    "# 단어별 빈도수 계산\n",
    "word_counts_normal = Counter(all_keywords_normal)\n",
    "\n",
    "# 상위 20개 키워드 출력\n",
    "print(\"\\n--- 정상 대화 키워드 빈도수 (상위 20개) ---\")\n",
    "print(word_counts_normal.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94827d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 정상 대화 상위 700개 키워드 (상위 10개) ---\n",
      "  Keyword  Frequency  Proportion(%)\n",
      "0       하      71818          4.957\n",
      "1       되      60325          4.164\n",
      "2       있      37434          2.584\n",
      "3      확인      28242          1.949\n",
      "4       같      20316          1.402\n",
      "5      교재      18717          1.292\n",
      "6      감사      18297          1.263\n",
      "7      환불      17476          1.206\n",
      "8      드리      17093          1.180\n",
      "9      회원      16150          1.115\n",
      "\n",
      "'top_700_keywords_normal_no_stopwords.csv' 파일 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 빈도수 상위 700개 키워드 추출\n",
    "top_700_keywords_normal = word_counts_normal.most_common(700)\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "top_700_df_normal = pd.DataFrame(\n",
    "    top_700_keywords_normal, columns=[\"Keyword\", \"Frequency\"]\n",
    ")\n",
    "\n",
    "# 비율(%) 컬럼 추가\n",
    "top_700_df_normal[\"Proportion(%)\"] = (\n",
    "    top_700_df_normal[\"Frequency\"] / total_keyword_count_normal * 100\n",
    ").round(3)\n",
    "\n",
    "# CSV 파일로 저장 (파일 이름 변경)\n",
    "file_name_to_save = \"top_700_keywords_normal_no_stopwords.csv\"\n",
    "top_700_df_normal.to_csv(file_name_to_save, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n--- 정상 대화 상위 700개 키워드 (상위 10개) ---\")\n",
    "print(top_700_df_normal.head(10))\n",
    "\n",
    "print(f\"\\n'{file_name_to_save}' 파일 저장이 완료되었습니다.\")"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "markdown",
   "id": "8e7acf6f",
   "metadata": {},
   "source": [
    "# 형태소 분리기 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b14933c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran_df = pd.read_csv(\"komoran_700_nvp_keywords.csv\")\n",
    "okt_df = pd.read_csv(\"0kt_700_nvp_keywords.csv\")\n",
    "kiwi_df = pd.read_csv(\"kiwi_700_nvp_keywords.csv\")\n",
    "kkma_df = pd.read_csv(\"kkma_700_nvp_keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4070d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Komoran 원본 ---\n",
      "  keyword  frequency     ratio\n",
      "0       하      67207  3.985840\n",
      "1       되      57743  3.424559\n",
      "2       있      30882  1.831516\n",
      "3      없음      28574  1.694636\n",
      "4      확인      28223  1.673819\n",
      "\n",
      "--- Okt 원본 ---\n",
      "  keyword  frequency\n",
      "0      하다     148246\n",
      "1       네      86406\n",
      "2      되다      61282\n",
      "3       그      49972\n",
      "4      있다      48927\n",
      "\n",
      "--- Kiwi 원본 ---\n",
      "  keyword  frequency  Proportion(%)\n",
      "0       하      71818          4.957\n",
      "1       되      60325          4.164\n",
      "2       있      37434          2.584\n",
      "3      확인      28242          1.949\n",
      "4       같      20316          1.402\n",
      "\n",
      "--- Kkma 원본 ---\n",
      "  keyword  frequency\n",
      "0       하      75408\n",
      "1       되      58728\n",
      "2       아      40011\n",
      "3       있      37516\n",
      "4       어      29153\n"
     ]
    }
   ],
   "source": [
    "# 각 데이터프레임의 구조를 확인합니다.\n",
    "print(\"--- Komoran 원본 ---\")\n",
    "print(komoran_df.head())\n",
    "\n",
    "print(\"\\n--- Okt 원본 ---\")\n",
    "print(okt_df.head())\n",
    "\n",
    "print(\"\\n--- Kiwi 원본 ---\")\n",
    "print(kiwi_df.head())\n",
    "\n",
    "print(\"\\n--- Kkma 원본 ---\")\n",
    "print(kkma_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30c8cce4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\woogawooga\\woogawooga_project\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'word'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m komoran_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkomoran\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      2\u001b[0m     komoran_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m komoran_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n\u001b[1;32m----> 4\u001b[0m okt_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mokt_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m okt_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m kiwi_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkiwi\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      6\u001b[0m     kiwi_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m kiwi_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m kkma_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkkma\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      9\u001b[0m     kkma_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m kkma_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\woogawooga\\woogawooga_project\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\woogawooga\\woogawooga_project\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'word'"
     ]
    }
   ],
   "source": [
    "komoran_df[\"komoran\"] = (\n",
    "    komoran_df[\"keyword\"].astype(str) + \" (\" + komoran_df[\"frequency\"].astype(str) + \")\"\n",
    ")\n",
    "okt_df[\"okt\"] = okt_df[\"word\"].astype(str) + \" (\" + okt_df[\"count\"].astype(str) + \")\"\n",
    "kiwi_df[\"kiwi\"] = (\n",
    "    kiwi_df[\"Keyword\"].astype(str) + \" (\" + kiwi_df[\"Frequency\"].astype(str) + \")\"\n",
    ")\n",
    "kkma_df[\"kkma\"] = (\n",
    "    kkma_df[\"keyword\"].astype(str) + \" (\" + kkma_df[\"frequency\"].astype(str) + \")\"\n",
    ")\n",
    "\n",
    "\n",
    "combined_df = pd.concat(\n",
    "    [komoran_df[[\"komoran\"]], okt_df[[\"okt\"]], kiwi_df[[\"kiwi\"]], kkma_df[[\"kkma\"]]],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "# 4. 최종 결과 확인 및 저장\n",
    "print(\"--- 최종 비교 결과 ---\")\n",
    "print(combined_df.head())\n",
    "\n",
    "combined_df.to_csv(\n",
    "    \"combined_keywords_analysis_final.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(\"\\n'combined_keywords_analysis_final.csv' 파일로 저장이 완료되었습니다.\")"
   ]
  },
  {
>>>>>>> ddf91fa048d35a4ac935fe2ebcebf5ee89bcb4f5
   "cell_type": "code",
   "execution_count": null,
   "id": "afa23319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "woogawooga-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
