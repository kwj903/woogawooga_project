{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\woogawooga\\woogawooga_project\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "import whisperx\n",
    "from pyannote.audio import Pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffa12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"환경 설정을 시작합니다...\")\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"❌ .env 파일에 HF_TOKEN이 설정되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# WhisperX와 Pyannote를 활용한 STT, 화자 분리, 역할 분류 자동화 스크립트\n",
    "# ==================================================================================\n",
    "#\n",
    "# [ 주요 기능 ]\n",
    "# 1. 오디오 전처리: FFmpeg을 사용하여 모든 오디오 파일(예: MP3)을 STT 모델에 적합한\n",
    "#    16kHz 모노 WAV 파일로 자동 변환합니다.\n",
    "#\n",
    "# 2. 고성능 STT 및 화자 분리:\n",
    "#    - `WhisperX`: OpenAI의 Whisper 모델을 기반으로 빠르고 정확한 음성 인식(STT) 및\n",
    "#      단어 단위 시간 정렬(Word-level Alignment)을 수행합니다.\n",
    "#    - `Pyannote.audio`: Hugging Face의 사전 학습된 모델을 사용하여 오디오에 포함된\n",
    "#      화자들을 분리(Diarization)합니다. (최대/최소 2명으로 설정)\n",
    "#\n",
    "# 3. 키워드 기반 역할 분류:\n",
    "#    - 분리된 화자의 발화 내용에 특정 키워드('대출', '검찰' 등)가 포함되어 있는지 여부를\n",
    "#      판단하여 '피싱범'과 '피해자' 역할을 자동으로 분류합니다.\n",
    "#\n",
    "# 4. 결과 통합 및 저장:\n",
    "#    - 모든 처리 결과를 결합하여 [역할, 화자, 시작시간, 발화내용] 형태로 터미널에 출력합니다.\n",
    "#    - 최종 결과를 원본 파일명에 기반한 CSV 파일로 깔끔하게 저장합니다.\n",
    "#\n",
    "# [ 사전 준비 사항 ]\n",
    "# 1. FFmpeg 설치: 시스템에 FFmpeg이 설치되어 있어야 오디오 변환이 가능합니다.\n",
    "# 2. Hugging Face Token: Pyannote 모델 사용을 위해 Hugging Face의 인증 토큰이\n",
    "#    필요합니다. (코드 내 `HF_TOKEN` 변수 설정)\n",
    "#\n",
    "# [ 사용 방법 ]\n",
    "# 1. `dataset` 폴더를 만들고 그 안에 분석할 오디오 파일을 넣습니다.\n",
    "# 2. 코드 내 `AUDIO_FILENAME` 변수에 해당 파일명을 정확히 입력합니다.\n",
    "# 3. 스크립트를 실행하면 `output` 폴더에 변환된 WAV 파일과 최종 CSV 결과물이 생성됩니다.\n",
    "#\n",
    "# ==================================================================================\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"사용할 장치: {DEVICE}\")\n",
    "\n",
    "\n",
    "# 데이터 및 결과 폴더 설정\n",
    "\n",
    "DATASET_DIR = \"dataset\"\n",
    "\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)  # 결과 폴더 자동 생성\n",
    "\n",
    "print(f\"✅ 데이터 폴더: '{DATASET_DIR}', 결과 폴더: '{OUTPUT_DIR}'\")\n",
    "\n",
    "\n",
    "# 처리할 오디오 파일 경로 (dataset 폴더 안에 있어야 함)\n",
    "\n",
    "AUDIO_FILENAME = \"1. 기존 대출금 일부 변제해야 저금리 대출 가능(햇살론 사칭)_.mp3\"\n",
    "\n",
    "audio_path = os.path.join(DATASET_DIR, AUDIO_FILENAME)\n",
    "\n",
    "\n",
    "if not os.path.exists(audio_path):\n",
    "\n",
    "    raise FileNotFoundError(f\"오디오 파일을 찾을 수 없습니다: {audio_path}\")\n",
    "\n",
    "print(f\"처리할 오디오 파일: {audio_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. 오디오 파일 WAV로 변환 ---\n",
    "\n",
    "def convert_to_wav(input_path, output_dir, sample_rate=16000):\n",
    "\n",
    "    \"\"\"MP3 파일을 STT에 적합한 16kHz 모노 WAV 파일로 변환합니다.\"\"\"\n",
    "\n",
    "    filename_without_ext = os.path.splitext(os.path.basename(input_path))[0]\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"{filename_without_ext}_converted.wav\")\n",
    "\n",
    "\n",
    "    print(f\"\\n🔹 '{input_path}'를 WAV 파일로 변환 중...\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        # ffmpeg을 사용하여 변환 (ffmpeg 설치 필요)\n",
    "\n",
    "        cmd = [\n",
    "\n",
    "            \"ffmpeg\",\n",
    "\n",
    "            \"-y\",\n",
    "\n",
    "            \"-i\",\n",
    "\n",
    "            input_path,\n",
    "\n",
    "            \"-ac\",\n",
    "\n",
    "            \"1\",  # 모노 채널\n",
    "\n",
    "            \"-ar\",\n",
    "\n",
    "            str(sample_rate),  # 16kHz 샘플레이트\n",
    "\n",
    "            output_path,\n",
    "\n",
    "        ]\n",
    "\n",
    "        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        print(f\"WAV 변환 완료: {output_path}\")\n",
    "\n",
    "        return output_path\n",
    "\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
    "        print(\n",
    "\n",
    "            \"'ffmpeg'이 설치되어 있는지 확인해주세요. ffmpeg을 사용하여 오디오 파일을 변환하는 데 실패했습니다.\"\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "\n",
    "\n",
    "wav_path = convert_to_wav(audio_path, OUTPUT_DIR)\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. WhisperX STT 및 화자 분리 실행 ---\n",
    "\n",
    "print(\"\\n WhisperX 모델 로딩 및 음성 인식 시작...\")\n",
    "\n",
    "# 모델 로드 (CPU 사용 시 float32, GPU 사용 시 float16 또는 bfloat16 권장)\n",
    "\n",
    "model = whisperx.load_model(\n",
    "\n",
    "    \"large-v2\", DEVICE, compute_type=\"float32\" if DEVICE == \"cpu\" else \"float16\"\n",
    ")\n",
    "\n",
    "\n",
    "# 1. 음성 인식 (Transcribe)\n",
    "\n",
    "stt_result = model.transcribe(wav_path, language=\"ko\")\n",
    "\n",
    "\n",
    "# 2. 단어 시간 정렬 (Align)\n",
    "\n",
    "print(\"단어 시간 정렬 중...\")\n",
    "\n",
    "model_a, metadata = whisperx.load_align_model(language_code=\"ko\", device=DEVICE)\n",
    "\n",
    "aligned_result = whisperx.align(\n",
    "\n",
    "    stt_result[\"segments\"], model_a, metadata, wav_path, DEVICE\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Pyannote 화자 분리 (Diarize)\n",
    "\n",
    "print(\"Pyannote 화자 분리 모델 로딩 및 실행 중...\")\n",
    "\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "\n",
    "    \"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "\n",
    "    diarization_pipeline.to(torch.device(DEVICE))  # GPU 사용 설정\n",
    "\n",
    "\n",
    "diarization_result = diarization_pipeline(wav_path, min_speakers=2, max_speakers=2)\n",
    "\n",
    "\n",
    "# 4. STT 결과와 화자 분리 결과 결합\n",
    "\n",
    "print(\"STT와 화자 분리 결과 결합 중...\")\n",
    "\n",
    "final_result = whisperx.assign_word_speakers(diarization_result, aligned_result)\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. 키워드 기반 역할 분류 및 결과 저장 ---\n",
    "\n",
    "print(\"\\n역할 분류 및 최종 결과 생성...\")\n",
    "\n",
    "\n",
    "# 키워드 설정 (필요에 따라 수정)\n",
    "\n",
    "PHISHER_KEYWORDS = [\n",
    "\n",
    "    \"대출\",\n",
    "\n",
    "    \"검찰\",\n",
    "\n",
    "    \"경찰\",\n",
    "\n",
    "    \"송금\",\n",
    "\n",
    "    \"계좌\",\n",
    "\n",
    "    \"수사관\",\n",
    "\n",
    "    \"저금리\",\n",
    "\n",
    "    \"햇살론\",\n",
    "\n",
    "]\n",
    "\n",
    "VICTIM_KEYWORDS = [\"네\", \"제가\", \"저\", \"어떻게\", \"진짜요\", \"몰랐어요\"]\n",
    "\n",
    "\n",
    "results_for_csv = []\n",
    "\n",
    "current_speaker = None\n",
    "\n",
    "current_text = \"\"\n",
    "\n",
    "current_role = \"알 수 없음\"\n",
    "\n",
    "start_time = 0\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "print(\"              대화 내용            \")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "for segment in final_result.get(\"segments\", []):\n",
    "\n",
    "    speaker = segment.get(\"speaker\", \"UNKNOWN\")\n",
    "\n",
    "    text = segment.get(\"text\", \"\").strip()\n",
    "\n",
    "\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "\n",
    "    # 화자가 바뀌면 이전 대화 내용 처리\n",
    "\n",
    "    if speaker != current_speaker and current_speaker is not None:\n",
    "\n",
    "        # 역할 분류\n",
    "\n",
    "        if any(kw in current_text for kw in PHISHER_KEYWORDS):\n",
    "\n",
    "            current_role = \"피싱범\"\n",
    "\n",
    "        elif any(kw in current_text for kw in VICTIM_KEYWORDS):\n",
    "\n",
    "            current_role = \"피해자\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            current_role = f\"{current_speaker}(역할 불분명)\"\n",
    "\n",
    "\n",
    "        # 결과 저장\n",
    "\n",
    "        entry = {\n",
    "\n",
    "            \"역할\": current_role,\n",
    "\n",
    "            \"화자\": current_speaker,\n",
    "\n",
    "            \"시작시간\": f\"{start_time:.2f}s\",\n",
    "\n",
    "            \"발화내용\": current_text,\n",
    "\n",
    "        }\n",
    "\n",
    "        results_for_csv.append(entry)\n",
    "\n",
    "\n",
    "        # 터미널 출력\n",
    "        print(\n",
    "\n",
    "            f\"[{entry['역할']} / {entry['화자']}] ({entry['시작시간']}): {entry['발화내용']}\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # 초기화\n",
    "\n",
    "        current_text = \"\"\n",
    "\n",
    "\n",
    "    # 현재 대화 내용 업데이트\n",
    "\n",
    "    if current_speaker is None or speaker != current_speaker:\n",
    "\n",
    "        current_speaker = speaker\n",
    "\n",
    "        start_time = segment.get(\"start\", 0)\n",
    "\n",
    "\n",
    "    current_text += text + \" \"\n",
    "\n",
    "\n",
    "# 마지막 대화 내용 처리\n",
    "\n",
    "if current_text:\n",
    "\n",
    "    if any(kw in current_text for kw in PHISHER_KEYWORDS):\n",
    "\n",
    "        current_role = \"피싱범\"\n",
    "\n",
    "    elif any(kw in current_text for kw in VICTIM_KEYWORDS):\n",
    "\n",
    "        current_role = \"피해자\"\n",
    "\n",
    "    else:\n",
    "\n",
    "        current_role = f\"{current_speaker}(역할 불분명)\"\n",
    "\n",
    "\n",
    "    entry = {\n",
    "\n",
    "        \"역할\": current_role,\n",
    "\n",
    "        \"화자\": current_speaker,\n",
    "\n",
    "        \"시작시간\": f\"{start_time:.2f}s\",\n",
    "\n",
    "        \"발화내용\": current_text,\n",
    "\n",
    "    }\n",
    "\n",
    "    results_for_csv.append(entry)\n",
    "    print(\n",
    "\n",
    "        f\"[{entry['역할']} / {entry['화자']}] ({entry['시작시간']}): {entry['발화내용']}\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# CSV 파일로 저장\n",
    "\n",
    "csv_filename = os.path.splitext(AUDIO_FILENAME)[0] + \"_dialogue.csv\"\n",
    "\n",
    "csv_path = os.path.join(OUTPUT_DIR, csv_filename)\n",
    "\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"역할\", \"화자\", \"시작시간\", \"발화내용\"])\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    writer.writerows(results_for_csv)\n",
    "\n",
    "\n",
    "print(f\"\\n모든 작업 완료! 최종 결과가 다음 파일에 저장되었습니다: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "woogawooga_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
