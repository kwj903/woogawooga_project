{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65dd244",
   "metadata": {},
   "source": [
    "# kiwi로 토큰화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0036d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반방법 : 데이터 불균형 고려안함, N그램 안씀, k폴드교차검증 안함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ea708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_csv(\"../../dataset/merged_labeled_data.csv\")\n",
    "\n",
    "# Kiwi 객체 생성\n",
    "kiwi = Kiwi()\n",
    "\n",
    "\n",
    "# 형태소 분석 함수 정의\n",
    "def tokenize_and_filter(text):\n",
    "    result = kiwi.analyze(text)[0][0]  # 첫 번째 결과의 토큰 리스트 사용\n",
    "    tokens = []\n",
    "    for word, pos, _, _ in result:\n",
    "        if pos in {\"NNG\", \"NNP\", \"VV\", \"VA\"}:  # 일반명사, 고유명사, 동사, 형용사\n",
    "            if pos in {\"VV\", \"VA\"}:\n",
    "                word = word + \"다\"  # 동사/형용사는 어간 + '다'로 표제어 처리\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 텍스트 생성\n",
    "df[\"tokens\"] = df[\"text\"].astype(str).apply(tokenize_and_filter)\n",
    "df[\"joined_tokens\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"joined_tokens\"])\n",
    "y = df[\"is_phishing\"]\n",
    "\n",
    "# 학습/테스트 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 간단한 분류 모델 (로지스틱 회귀)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, output_dict=False)\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1e977",
   "metadata": {},
   "source": [
    "# k 폴드 교차검증 방식, n그램사용, 데이터 불균형 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 다시 로드 (중복 방지)\n",
    "df = pd.read_csv(\"../../dataset/merged_labeled_data.csv\")\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "kiwi = Kiwi()\n",
    "\n",
    "\n",
    "# 형태소 분석 함수 (명사/동사/형용사/고유명사, 표제어 처리 포함)\n",
    "def tokenize_and_filter(text):\n",
    "    result = kiwi.analyze(text)[0][0]\n",
    "    tokens = []\n",
    "    for word, pos, _, _ in result:\n",
    "        if pos in {\"NNG\", \"NNP\", \"VV\", \"VA\"}:\n",
    "            if pos in {\"VV\", \"VA\"}:\n",
    "                word = word + \"다\"\n",
    "            tokens.append(word)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# 텍스트 전처리\n",
    "df[\"processed_text\"] = df[\"text\"].astype(str).apply(tokenize_and_filter)\n",
    "\n",
    "X = df[\"processed_text\"]\n",
    "y = df[\"is_phishing\"]\n",
    "\n",
    "# 파이프라인 구성: TF-IDF + 로지스틱 회귀 (클래스 불균형 고려, n-gram 적용)\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                sublinear_tf=True,\n",
    "                min_df=5,\n",
    "                max_df=0.9,\n",
    "                ngram_range=(1, 2),  # uni-gram + bi-gram\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 교차 검증 예측\n",
    "y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "# 평가 리포트 출력\n",
    "report = classification_report(y, y_pred)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b208ca1",
   "metadata": {},
   "source": [
    "# 위의 두 방식을 종합하여 비교까지 진행하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c789014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# 샘플 데이터 로드 경로 수정 필요 시 여기를 변경하세요\n",
    "data_path = \"../../dataset/merged_labeled_data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Kiwi 형태소 분석기\n",
    "kiwi = Kiwi()\n",
    "\n",
    "\n",
    "# 토큰화 및 표제어 처리\n",
    "def tokenize_and_filter(text):\n",
    "    result = kiwi.analyze(text)[0][0]\n",
    "    tokens = []\n",
    "    for word, pos, _, _ in result:\n",
    "        if pos in {\"NNG\", \"NNP\", \"VV\", \"VA\"}:\n",
    "            if pos in {\"VV\", \"VA\"}:\n",
    "                word = word + \"다\"\n",
    "            tokens.append(word)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# 텍스트 전처리\n",
    "df[\"processed_text\"] = df[\"text\"].astype(str).apply(tokenize_and_filter)\n",
    "\n",
    "X = df[\"processed_text\"]\n",
    "y = df[\"is_phishing\"]\n",
    "\n",
    "# ▶ 방식 1: 단순 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipeline_simple = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.9)),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\")),\n",
    "    ]\n",
    ")\n",
    "pipeline_simple.fit(X_train, y_train)\n",
    "y_pred_simple = pipeline_simple.predict(X_test)\n",
    "report_simple = classification_report(y_test, y_pred_simple, output_dict=True)\n",
    "\n",
    "# ▶ 방식 2: K-Fold 교차 검증 + ngram 적용\n",
    "pipeline_kfold = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                sublinear_tf=True, min_df=5, max_df=0.9, ngram_range=(1, 2)\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\")),\n",
    "    ]\n",
    ")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_pred_kfold = cross_val_predict(pipeline_kfold, X, y, cv=skf)\n",
    "report_kfold = classification_report(y, y_pred_kfold, output_dict=True)\n",
    "\n",
    "# 결과 비교를 위해 정리\n",
    "comparison = {\"Metric\": [], \"Simple Split\": [], \"K-Fold\": []}\n",
    "\n",
    "for label in [\"0\", \"1\", \"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "    if label == \"accuracy\":\n",
    "        comparison[\"Metric\"].append(\"Accuracy\")\n",
    "        comparison[\"Simple Split\"].append(report_simple[\"accuracy\"])\n",
    "        comparison[\"K-Fold\"].append(report_kfold[\"accuracy\"])\n",
    "    else:\n",
    "        for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "            metric_name = (\n",
    "                f\"{label} {metric}\" if label in [\"0\", \"1\"] else f\"{label} {metric}\"\n",
    "            )\n",
    "            comparison[\"Metric\"].append(metric_name)\n",
    "            comparison[\"Simple Split\"].append(report_simple[label][metric])\n",
    "            comparison[\"K-Fold\"].append(report_kfold[label][metric])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886f5621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Simple Split</th>\n",
       "      <th>K-Fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 precision</td>\n",
       "      <td>0.999161</td>\n",
       "      <td>0.996500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0 recall</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>0.996500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 f1-score</td>\n",
       "      <td>0.995819</td>\n",
       "      <td>0.996500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 precision</td>\n",
       "      <td>0.978469</td>\n",
       "      <td>0.989741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 recall</td>\n",
       "      <td>0.997561</td>\n",
       "      <td>0.989741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1 f1-score</td>\n",
       "      <td>0.987923</td>\n",
       "      <td>0.989741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.993789</td>\n",
       "      <td>0.994781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>macro avg precision</td>\n",
       "      <td>0.988815</td>\n",
       "      <td>0.993121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>macro avg recall</td>\n",
       "      <td>0.995030</td>\n",
       "      <td>0.993121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>macro avg f1-score</td>\n",
       "      <td>0.991871</td>\n",
       "      <td>0.993121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>weighted avg precision</td>\n",
       "      <td>0.993892</td>\n",
       "      <td>0.994781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted avg recall</td>\n",
       "      <td>0.993789</td>\n",
       "      <td>0.994781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>weighted avg f1-score</td>\n",
       "      <td>0.993808</td>\n",
       "      <td>0.994781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Metric  Simple Split    K-Fold\n",
       "0              0 precision      0.999161  0.996500\n",
       "1                 0 recall      0.992500  0.996500\n",
       "2               0 f1-score      0.995819  0.996500\n",
       "3              1 precision      0.978469  0.989741\n",
       "4                 1 recall      0.997561  0.989741\n",
       "5               1 f1-score      0.987923  0.989741\n",
       "6                 Accuracy      0.993789  0.994781\n",
       "7      macro avg precision      0.988815  0.993121\n",
       "8         macro avg recall      0.995030  0.993121\n",
       "9       macro avg f1-score      0.991871  0.993121\n",
       "10  weighted avg precision      0.993892  0.994781\n",
       "11     weighted avg recall      0.993789  0.994781\n",
       "12   weighted avg f1-score      0.993808  0.994781"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Metric  Simple Split    K-Fold\n",
      "0              0 precision      0.999161  0.996500\n",
      "1                 0 recall      0.992500  0.996500\n",
      "2               0 f1-score      0.995819  0.996500\n",
      "3              1 precision      0.978469  0.989741\n",
      "4                 1 recall      0.997561  0.989741\n",
      "5               1 f1-score      0.987923  0.989741\n",
      "6                 Accuracy      0.993789  0.994781\n",
      "7      macro avg precision      0.988815  0.993121\n",
      "8         macro avg recall      0.995030  0.993121\n",
      "9       macro avg f1-score      0.991871  0.993121\n",
      "10  weighted avg precision      0.993892  0.994781\n",
      "11     weighted avg recall      0.993789  0.994781\n",
      "12   weighted avg f1-score      0.993808  0.994781\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "result_df = pd.DataFrame(comparison)\n",
    "display(result_df)\n",
    "\n",
    "# 콘솔에서 직접 출력\n",
    "print(result_df)\n",
    "\n",
    "# 또는 CSV 파일로 저장\n",
    "result_df.to_csv(\"../../dataset/model_comparison_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4dfa5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../models/pipeline_kfold.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 저장\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 모델 저장 디렉토리 만들기\n",
    "os.makedirs(\"../../models\", exist_ok=True)\n",
    "\n",
    "# 1. Simple Split 모델 저장\n",
    "joblib.dump(pipeline_simple, \"../../models/pipeline_simple.pkl\")\n",
    "\n",
    "# 2. K-Fold 모델 → 전체 데이터로 다시 학습 후 저장\n",
    "pipeline_kfold.fit(X, y)\n",
    "joblib.dump(pipeline_kfold, \"../../models/pipeline_kfold.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc87701c",
   "metadata": {},
   "source": [
    "# 피싱 논피싱 단순 판별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9526b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장된 모델 불러오기\n",
    "loaded_model = joblib.load(\"../../models/1차모델_원본데이터_pipeline_Ngram_kfold.pkl\")\n",
    "loaded_model.predict([\"수사기관을 사칭하고 계좌를 요구하는 대화입니다\"])  # 판별할 문장을 넣는곳"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303c9ed",
   "metadata": {},
   "source": [
    "# 퍼센테이지로 구간 별로 판별함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad870532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "import joblib\n",
    "\n",
    "loaded_model = joblib.load(\"../../models/1차모델_원본데이터_pipeline_Ngram_kfold.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26eca76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_hold(prob, low=0.3, high=0.7):\n",
    "    if prob < low:\n",
    "        return 0  # 일반 대화\n",
    "    elif prob > high:\n",
    "        return 1  # 보이스피싱\n",
    "    else:\n",
    "        return -1  # 보류 → 2차 모델로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 확률: 0.6084\n",
      "1차 판별 결과: -1 (0: 일반, 1: 피싱, -1: 보류)\n"
     ]
    }
   ],
   "source": [
    "# 예측 대상 문장\n",
    "input_text = [\n",
    "    \"그러시면 71년생 남성 사람은 아십니까? 지금 강성호 씨는 돈을 주고 샀다고 짓을 하는데 본인께서 감상으로 저녁 먹읍시다 말씀이세요. 알다시피 금융거래실명법 원칙에 따라 통장을 개설하면 본인이 직접 신분증을 지참하고 은행에 가서 하지 마 통장 발급 되지 않습니까네? 본인이 모른다고 해서 5만 원은 살 거는 아니고요. 보이 입장도 이해를 하고 환경을 수 없지만요. 이번 사건은 방송을 하는 사람이 불법도박사이트로 운영하면서 대포통장이 필요하니까 50원을 포함한 전국의 132명의 자들에게 통장을 구매했다고 짓을 해서 연락을 드리면 부분이고요.\"\n",
    "]  \n",
    "\n",
    "# 보이스피싱(클래스 1) 확률만 추출\n",
    "proba = loaded_model.predict_proba(input_text)[0][1]\n",
    "\n",
    "# 보류 구간 판별 적용\n",
    "final_result = classify_with_hold(proba)\n",
    "\n",
    "print(f\"예측 확률: {proba:.4f}\")\n",
    "print(f\"1차 판별 결과: {final_result} (0: 일반, 1: 피싱, -1: 보류)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d824a3",
   "metadata": {},
   "source": [
    "# 앙상블 Bagging 1차 모델 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504efb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6000\n",
      "           1       1.00      0.96      0.98      2047\n",
      "\n",
      "    accuracy                           0.99      8047\n",
      "   macro avg       0.99      0.98      0.99      8047\n",
      "weighted avg       0.99      0.99      0.99      8047\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../../models/1차모델_원본데이터_pipeline_rf.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kiwipiepy import Kiwi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = \"../../dataset/merged_labeled_data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Kiwi 토큰화 + 표제어 처리\n",
    "kiwi = Kiwi()\n",
    "\n",
    "\n",
    "def tokenize_and_filter(text):\n",
    "    result = kiwi.analyze(text)[0][0]\n",
    "    tokens = []\n",
    "    for word, pos, _, _ in result:\n",
    "        if pos in {\"NNG\", \"NNP\", \"VV\", \"VA\"}:\n",
    "            if pos in {\"VV\", \"VA\"}:\n",
    "                word = word + \"다\"\n",
    "            tokens.append(word)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "df[\"processed_text\"] = df[\"text\"].astype(str).apply(tokenize_and_filter)\n",
    "\n",
    "X = df[\"processed_text\"]\n",
    "y = df[\"is_phishing\"]\n",
    "\n",
    "# ▶ 앙상블 모델 파이프라인 (TF-IDF + RandomForest)\n",
    "pipeline_rf = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                sublinear_tf=True, min_df=5, max_df=0.9, ngram_range=(1, 2)\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"clf\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=100, random_state=42, class_weight=\"balanced\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ▶ K-Fold 교차 검증 성능 측정\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_pred_rf = cross_val_predict(pipeline_rf, X, y, cv=skf)\n",
    "\n",
    "# ▶ 성능 리포트 출력\n",
    "report_rf = classification_report(y, y_pred_rf, output_dict=False)\n",
    "print(report_rf)\n",
    "\n",
    "# ▶ 전체 데이터로 다시 학습 후 저장\n",
    "pipeline_rf.fit(X, y)\n",
    "\n",
    "# 저장 경로\n",
    "os.makedirs(\"../../models\", exist_ok=True)\n",
    "joblib.dump(pipeline_rf, \"../../models/1차모델_원본데이터_pipeline_rf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd199ff",
   "metadata": {},
   "source": [
    "# 앙상블 Stacking 1차 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3b759a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6000\n",
      "           1       1.00      0.96      0.98      2047\n",
      "\n",
      "    accuracy                           0.99      8047\n",
      "   macro avg       0.99      0.98      0.99      8047\n",
      "weighted avg       0.99      0.99      0.99      8047\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# 8. 최종 전체 학습 및 저장\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m \u001b[43mpipeline_stacking\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../models\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     86\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(pipeline_stacking, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../models/pipeline_stacking.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\data_project\\woogawooga_project\\.venv\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\data_project\\woogawooga_project\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:661\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    656\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    657\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    658\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    659\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    660\u001b[0m         )\n\u001b[1;32m--> 661\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\data_project\\woogawooga_project\\.venv\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py:706\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    704\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\data_project\\woogawooga_project\\.venv\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\data_project\\woogawooga_project\\.venv\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py:211\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(estimator)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m    220\u001b[0m est_fitted_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\data_project\\woogawooga_project\\.venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\data_project\\woogawooga_project\\.venv\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\data_project\\woogawooga_project\\.venv\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\Downloads\\data_project\\woogawooga_project\\.venv\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kiwipiepy import Kiwi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.ensemble import (\n",
    "    StackingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "df = pd.read_csv(\"../../dataset/merged_labeled_data.csv\")\n",
    "\n",
    "# 2. 형태소 분석기 설정\n",
    "kiwi = Kiwi()\n",
    "\n",
    "\n",
    "def tokenize_and_filter(text):\n",
    "    result = kiwi.analyze(text)[0][0]\n",
    "    tokens = []\n",
    "    for word, pos, _, _ in result:\n",
    "        if pos in {\"NNG\", \"NNP\", \"VV\", \"VA\"}:\n",
    "            if pos in {\"VV\", \"VA\"}:\n",
    "                word += \"다\"\n",
    "            tokens.append(word)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# 3. 전처리\n",
    "df[\"processed_text\"] = df[\"text\"].astype(str).apply(tokenize_and_filter)\n",
    "X = df[\"processed_text\"]\n",
    "y = df[\"is_phishing\"]\n",
    "\n",
    "# 4. 스태킹 모델 구성\n",
    "base_models = [\n",
    "    (\"lr\", LogisticRegression(max_iter=1000, class_weight=\"balanced\")),\n",
    "    (\n",
    "        \"rf\",\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=100, class_weight=\"balanced\", random_state=42\n",
    "        ),\n",
    "    ),\n",
    "    (\"gb\", GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "]\n",
    "\n",
    "meta_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "stacked_clf = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,  # 내부에서 K-Fold\n",
    "    n_jobs=-1,\n",
    "    passthrough=False,  # True면 base 모델의 입력도 같이 전달됨\n",
    ")\n",
    "\n",
    "# 5. 전체 파이프라인 (TF-IDF → 스태킹 모델)\n",
    "pipeline_stacking = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                sublinear_tf=True, min_df=5, max_df=0.9, ngram_range=(1, 2)\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", stacked_clf),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 6. K-Fold 기반 평가\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_pred_stack = cross_val_predict(pipeline_stacking, X, y, cv=skf)\n",
    "\n",
    "# 7. 성능 출력\n",
    "report = classification_report(y, y_pred_stack)\n",
    "print(report)\n",
    "\n",
    "# 8. 최종 전체 학습 및 저장\n",
    "pipeline_stacking.fit(X, y)\n",
    "os.makedirs(\"../../models\", exist_ok=True)\n",
    "joblib.dump(pipeline_stacking, \"../../models/pipeline_stacking.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd672e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "woogawooga_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
