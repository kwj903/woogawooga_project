{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d261c9d",
   "metadata": {},
   "source": [
    "이 스크립트는 'datas' 폴더 하위의 모든 JSON 파일에서 대화 내용을 추출하여\n",
    "하나의 CSV 파일로 만들거나 기존 파일에 추가하는 작업을 수행합니다.\n",
    "\n",
    "주요 기능:\n",
    "- 'datas' 폴더 내의 모든 JSON 파일을 재귀적으로 탐색합니다.\n",
    "- 각 JSON 파일에서 파일 이름, 카테고리, 하위 카테고리, 화자(speaker), 메시지(text)를 추출합니다.\n",
    "- 추출된 데이터를 pandas DataFrame으로 변환합니다.\n",
    "- 'dataset/normal_dataset.csv' 파일이 이미 존재하면 기존 데이터를 불러와 새로운 데이터와 병합합니다.\n",
    "- 중복된 데이터는 제거하고, file_name을 기준으로 오름차순 정렬한 후 최종적으로 'dataset' 폴더에 'normal_dataset.csv'라는 이름으로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "datas_dir = os.path.join(base_dir, \"datas\")\n",
    "dataset_dir = os.path.join(base_dir, \"dataset\")\n",
    "output_csv_path = os.path.join(dataset_dir, \"normal_dataset.csv\")\n",
    "\n",
    "# 데이터셋 폴더가 없으면 생성\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "# 컬럼 정의\n",
    "columns = [\"file_name\", \"category\", \"subcategory\", \"spk\", \"msg\"]\n",
    "\n",
    "# 기존 CSV 파일이 있으면 데이터 불러오기\n",
    "if os.path.exists(output_csv_path):\n",
    "    try:\n",
    "        existing_df = pd.read_csv(output_csv_path)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        existing_df = pd.DataFrame(columns=columns)\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# 모든 JSON 파일 경로 가져오기\n",
    "json_files = []\n",
    "for root, _, files in os.walk(datas_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            json_files.append(os.path.join(root, file))\n",
    "\n",
    "# 데이터를 저장할 리스트 초기화\n",
    "new_data = []\n",
    "\n",
    "# 각 JSON 파일을 순회하며 데이터 추출\n",
    "for file_path in json_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_data = json.load(f)\n",
    "            type_info = json_data.get(\"dataSet\", {}).get(\"typeInfo\", {})\n",
    "            category = type_info.get(\"category\")\n",
    "            subcategory = type_info.get(\"subcategory\")\n",
    "            dialogs = json_data.get(\"dataSet\", {}).get(\"dialogs\", [])\n",
    "\n",
    "            for dialog in dialogs:\n",
    "                speaker = dialog.get(\"speaker\")\n",
    "                message = dialog.get(\"text\")\n",
    "                if speaker and message:\n",
    "                    new_data.append(\n",
    "                        {\n",
    "                            \"file_name\": file_name,\n",
    "                            \"category\": category,\n",
    "                            \"subcategory\": subcategory,\n",
    "                            \"spk\": speaker,\n",
    "                            \"msg\": message,\n",
    "                        }\n",
    "                    )\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Could not decode JSON from {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "# 새로운 데이터가 있을 경우에만 처리\n",
    "if new_data:\n",
    "    new_df = pd.DataFrame(new_data, columns=columns)\n",
    "\n",
    "    # 기존 데이터와 새로운 데이터 병합\n",
    "    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "    # 중복 데이터 제거\n",
    "    combined_df.drop_duplicates(subset=columns, keep=\"last\", inplace=True)\n",
    "\n",
    "    # file_name을 기준으로 오름차순 정렬\n",
    "    combined_df.sort_values(by=\"file_name\", inplace=True)\n",
    "\n",
    "    # CSV 파일로 저장\n",
    "    combined_df.to_csv(output_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Successfully updated and sorted {output_csv_path}\")\n",
    "else:\n",
    "    # 새로운 데이터가 없더라도 기존 데이터를 정렬하여 다시 저장\n",
    "    if not existing_df.empty:\n",
    "        existing_df.sort_values(by=\"file_name\", inplace=True)\n",
    "        existing_df.to_csv(output_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"No new data. Existing data in {output_csv_path} has been sorted.\")\n",
    "    else:\n",
    "        print(\n",
    "            \"No new data was extracted and existing data is empty. The CSV file was not updated.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "datas_dir = os.path.join(base_dir, \"datas\", \"merged\")\n",
    "dataset_dir = os.path.join(base_dir, \"dataset\")\n",
    "output_csv_path = os.path.join(dataset_dir, \"normal_dataset.csv\")\n",
    "\n",
    "# 데이터셋 폴더가 없으면 생성\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "# 모든 JSON 파일 경로 가져오기\n",
    "json_files = []\n",
    "for root, _, files in os.walk(datas_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            json_files.append(os.path.join(root, file))\n",
    "\n",
    "# 데이터를 저장할 리스트 초기화\n",
    "all_data = []\n",
    "\n",
    "# 각 JSON 파일을 순회하며 데이터 추출\n",
    "for file_path in json_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "            info = json_data.get(\"info\", {})\n",
    "            category = info.get(\"speaker_emotion\")\n",
    "            subcategory = info.get(\"relation\")\n",
    "\n",
    "            dialogs = json_data.get(\"utterances\", [])\n",
    "\n",
    "            # 화자(role)를 숫자로 매핑하기 위한 딕셔너리와 카운터\n",
    "            speaker_map = {}\n",
    "            next_speaker_id = 1\n",
    "\n",
    "            for dialog in dialogs:\n",
    "                role = dialog.get(\"role\")\n",
    "                message = dialog.get(\"text\")\n",
    "\n",
    "                if role and message:\n",
    "                    # 새로운 화자일 경우, 맵에 추가\n",
    "                    if role not in speaker_map:\n",
    "                        speaker_map[role] = next_speaker_id\n",
    "                        next_speaker_id += 1\n",
    "\n",
    "                    spk = speaker_map[role]\n",
    "\n",
    "                    all_data.append(\n",
    "                        {\n",
    "                            \"file_name\": file_name,\n",
    "                            \"category\": category,\n",
    "                            \"subcategory\": subcategory,\n",
    "                            \"spk\": spk,\n",
    "                            \"msg\": message,\n",
    "                        }\n",
    "                    )\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Could not decode JSON from {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "# 새로운 데이터가 있을 경우에만 처리\n",
    "if all_data:\n",
    "    # DataFrame 생성\n",
    "    df = pd.DataFrame(\n",
    "        all_data, columns=[\"file_name\", \"category\", \"subcategory\", \"spk\", \"msg\"]\n",
    "    )\n",
    "\n",
    "    # CSV 파일로 저장\n",
    "    df.to_csv(output_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Successfully created {output_csv_path} with {len(df)} rows.\")\n",
    "else:\n",
    "    print(\"No data was extracted. The CSV file was not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "# 입력 폴더: 'datas'\n",
    "input_dir = os.path.join(base_dir, \"datas\")\n",
    "\n",
    "# 출력 폴더: 'dataset'\n",
    "output_dir = os.path.join(base_dir, \"dataset\")\n",
    "output_csv_path = os.path.join(output_dir, \"normal_dataset3.csv\")\n",
    "mapping_csv_path = os.path.join(output_dir, \"file_name_mapping.csv\")\n",
    "\n",
    "# 출력 폴더가 없으면 생성\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 모든 JSON 파일 경로 가져오기\n",
    "json_files = []\n",
    "if os.path.exists(input_dir):\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                json_files.append(os.path.join(root, file))\n",
    "else:\n",
    "    print(f\"Error: Input directory '{input_dir}' not found.\")\n",
    "\n",
    "# 데이터를 저장할 리스트 및 파일 매핑 정보 초기화\n",
    "all_data = []\n",
    "file_mapping = []\n",
    "file_counter = 1\n",
    "\n",
    "# 각 JSON 파일을 순회하며 데이터 추출\n",
    "for file_path in json_files:\n",
    "    original_file_name = os.path.basename(file_path)\n",
    "    simple_file_name = f\"file_{file_counter}\"\n",
    "    file_mapping.append(\n",
    "        {\"original_name\": original_file_name, \"simple_name\": simple_file_name}\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "            # 메타데이터 추출\n",
    "            category = json_data.get(\"Noise\", {}).get(\"Speaker1NoiseCategory\")\n",
    "            if not category:\n",
    "                category = json_data.get(\"info\", {}).get(\"category\")\n",
    "\n",
    "            # 화자(SpeakerNo)를 숫자로 매핑하기 위한 딕셔너리와 카운터\n",
    "            speaker_map = {}\n",
    "            next_speaker_id = 1\n",
    "\n",
    "            # 두 가지 다른 JSON 구조 처리\n",
    "            if \"Conversation\" in json_data:  # 자유대화 형식\n",
    "                conversation = json_data.get(\"Conversation\", [])\n",
    "                for utterance in conversation:\n",
    "                    speaker_no = utterance.get(\"SpeakerNo\")\n",
    "                    text = utterance.get(\"Text\")\n",
    "                    emotion_target = utterance.get(\"SpeakerEmotionTarget\")\n",
    "\n",
    "                    if speaker_no and text:\n",
    "                        if speaker_no not in speaker_map:\n",
    "                            speaker_map[speaker_no] = next_speaker_id\n",
    "                            next_speaker_id += 1\n",
    "                        spk = speaker_map[speaker_no]\n",
    "\n",
    "                        all_data.append(\n",
    "                            {\n",
    "                                \"file_name\": simple_file_name,\n",
    "                                \"category\": category,\n",
    "                                \"subcategory\": emotion_target,\n",
    "                                \"spk\": spk,\n",
    "                                \"msg\": text,\n",
    "                            }\n",
    "                        )\n",
    "            elif \"utterances\" in json_data:  # 공감형 대화 형식\n",
    "                info = json_data.get(\"info\", {})\n",
    "                subcategory = info.get(\"relation\")\n",
    "                utterances = json_data.get(\"utterances\", [])\n",
    "                for utterance in utterances:\n",
    "                    role = utterance.get(\"role\")\n",
    "                    message = utterance.get(\"text\")\n",
    "\n",
    "                    if role and message:\n",
    "                        if role not in speaker_map:\n",
    "                            speaker_map[role] = next_speaker_id\n",
    "                            next_speaker_id += 1\n",
    "                        spk = speaker_map[role]\n",
    "\n",
    "                        all_data.append(\n",
    "                            {\n",
    "                                \"file_name\": simple_file_name,\n",
    "                                \"category\": info.get(\"speaker_emotion\"),\n",
    "                                \"subcategory\": subcategory,\n",
    "                                \"spk\": spk,\n",
    "                                \"msg\": message,\n",
    "                            }\n",
    "                        )\n",
    "        file_counter += 1\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Could not decode JSON from {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "# 데이터가 있을 경우에만 처리\n",
    "if all_data:\n",
    "    # DataFrame 생성\n",
    "    df = pd.DataFrame(\n",
    "        all_data, columns=[\"file_name\", \"category\", \"subcategory\", \"spk\", \"msg\"]\n",
    "    )\n",
    "    mapping_df = pd.DataFrame(file_mapping)\n",
    "\n",
    "    # CSV 파일로 저장\n",
    "    df.to_csv(output_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    mapping_df.to_csv(mapping_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"Successfully created {output_csv_path} with {len(df)} rows.\")\n",
    "    print(f\"Successfully created mapping file: {mapping_csv_path}\")\n",
    "else:\n",
    "    print(\"No data was extracted. The CSV file was not created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "woogawooga-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
