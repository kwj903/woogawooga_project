{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaeaf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# --- 설정 ---\n",
    "SAMPLE_SIZE = 6000\n",
    "INPUT_FILENAME = \"normal_dataset(콜센터 상담).csv\"\n",
    "OUTPUT_FILENAME_PREFIX = \"stratified_sample_callcenter\"\n",
    "# --- 설정 끝 ---\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_file = os.path.join(\"dataset\", INPUT_FILENAME)\n",
    "output_file = os.path.join(\"dataset\", f\"{OUTPUT_FILENAME_PREFIX}_{SAMPLE_SIZE}.csv\")\n",
    "\n",
    "# 데이터 불러오기\n",
    "print(f\"'{input_file}'에서 데이터를 불러오는 중...\")\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(\"데이터 불러오기 완료.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}'을(를) 찾을 수 없습니다. 경로를 확인하세요.\")\n",
    "    exit()\n",
    "\n",
    "# 고유 file_name과 해당 category/subcategory 추출\n",
    "print(\"고유 file_name을 기준으로 데이터 집계 중...\")\n",
    "unique_files = (\n",
    "    df[[\"file_name\", \"category\", \"subcategory\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(f\"총 {len(unique_files)}개의 고유한 file_name을 찾았습니다.\")\n",
    "\n",
    "# 데이터가 요청된 샘플 수보다 적은 경우, 가능한 모든 데이터를 사용\n",
    "if len(unique_files) < SAMPLE_SIZE:\n",
    "    print(\n",
    "        f\"경고: 고유 file_name의 수가 {SAMPLE_SIZE}개({len(unique_files)}개)보다 적습니다.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"가능한 모든 {len(unique_files)}개의 고유 file_name을 사용하여 파일을 생성합니다.\"\n",
    "    )\n",
    "    actual_sample_size = len(unique_files)\n",
    "    output_file = os.path.join(\n",
    "        \"dataset\", f\"{OUTPUT_FILENAME_PREFIX}_{actual_sample_size}.csv\"\n",
    "    )\n",
    "    sampled_files_df = unique_files\n",
    "else:\n",
    "    actual_sample_size = SAMPLE_SIZE\n",
    "    # category와 subcategory를 합쳐서 계층화 키로 사용\n",
    "    unique_files[\"stratify_key\"] = (\n",
    "        unique_files[\"category\"].astype(str)\n",
    "        + \"_\"\n",
    "        + unique_files[\"subcategory\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # 계층적 샘플링 수행\n",
    "    print(f\"{actual_sample_size}개에 대한 계층적 샘플링 수행 중...\")\n",
    "    try:\n",
    "        sampled_files_df, _ = train_test_split(\n",
    "            unique_files,\n",
    "            train_size=actual_sample_size,\n",
    "            stratify=unique_files[\"stratify_key\"],\n",
    "            random_state=42,\n",
    "        )\n",
    "        print(\"샘플링 완료.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"샘플링 오류: {e}\")\n",
    "        print(\n",
    "            \"계층별 최소 샘플 수가 부족할 수 있습니다. stratify 없이 샘플링을 시도합니다.\"\n",
    "        )\n",
    "        sampled_files_df = unique_files.sample(n=actual_sample_size, random_state=42)\n",
    "\n",
    "# 샘플링된 file_name 리스트 추출\n",
    "sampled_file_names = sampled_files_df[\"file_name\"].tolist()\n",
    "\n",
    "# 원본 데이터프레임에서 샘플링된 file_name에 해당하는 모든 데이터 추출\n",
    "print(\"원본 데이터에서 샘플링된 데이터 추출 중...\")\n",
    "result_df = df[df[\"file_name\"].isin(sampled_file_names)]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "print(f\"결과를 '{output_file}'에 저장 중...\")\n",
    "result_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n처리 완료!\")\n",
    "print(f\"총 {len(result_df)}개의 행이 포함된 샘플 데이터가 생성되었습니다.\")\n",
    "print(f\"고유 file_name 개수: {len(sampled_file_names)}개\")\n",
    "print(f\"결과 파일: '{os.path.abspath(output_file)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# 파일 경로 설정\n",
    "# 스크립트가 프로젝트 루트에서 실행된다고 가정합니다.\n",
    "input_file = os.path.join(\"dataset\", \"normal_dataset3.csv\")\n",
    "output_file = os.path.join(\"dataset\", \"stratified_8000_sample.csv\")\n",
    "\n",
    "# 데이터 불러오기\n",
    "print(f\"'{input_file}'에서 데이터를 불러오는 중...\")\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(\"데이터 불러오기 완료.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}'을(를) 찾을 수 없습니다. 경로를 확인하세요.\")\n",
    "    exit()\n",
    "\n",
    "# 고유 file_name과 해당 category/subcategory 추출\n",
    "# file_name 당 category와 subcategory는 동일하다고 가정\n",
    "print(\"고유 file_name을 기준으로 데이터 집계 중...\")\n",
    "unique_files = (\n",
    "    df[[\"file_name\", \"category\", \"subcategory\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(f\"총 {len(unique_files)}개의 고유한 file_name을 찾았습니다.\")\n",
    "\n",
    "SAMPLE_SIZE = 8000\n",
    "\n",
    "# 데이터가 요청된 샘플 수보다 적은 경우, 가능한 모든 데이터를 사용\n",
    "if len(unique_files) < SAMPLE_SIZE:\n",
    "    print(\n",
    "        f\"경고: 고유 file_name의 수가 {SAMPLE_SIZE}개({len(unique_files)}개)보다 적습니다.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"가능한 모든 {len(unique_files)}개의 고유 file_name을 사용하여 파일을 생성합니다.\"\n",
    "    )\n",
    "    SAMPLE_SIZE = len(unique_files)\n",
    "    output_file = os.path.join(\"dataset\", f\"stratified_{SAMPLE_SIZE}_sample.csv\")\n",
    "    sampled_files_df = unique_files\n",
    "else:\n",
    "    # category와 subcategory를 합쳐서 계층화 키로 사용\n",
    "    unique_files[\"stratify_key\"] = (\n",
    "        unique_files[\"category\"].astype(str)\n",
    "        + \"_\"\n",
    "        + unique_files[\"subcategory\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # 계층적 샘플링 수행\n",
    "    print(f\"{SAMPLE_SIZE}개에 대한 계층적 샘플링 수행 중...\")\n",
    "    try:\n",
    "        sampled_files_df, _ = train_test_split(\n",
    "            unique_files,\n",
    "            train_size=SAMPLE_SIZE,\n",
    "            stratify=unique_files[\"stratify_key\"],\n",
    "            random_state=42,\n",
    "        )\n",
    "        print(\"샘플링 완료.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"샘플링 오류: {e}\")\n",
    "        print(\n",
    "            \"계층별 최소 샘플 수가 부족할 수 있습니다. stratify 없이 샘플링을 시도합니다.\"\n",
    "        )\n",
    "        sampled_files_df = unique_files.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "\n",
    "\n",
    "# 샘플링된 file_name 리스트 추출\n",
    "sampled_file_names = sampled_files_df[\"file_name\"].tolist()\n",
    "\n",
    "# 원본 데이터프레임에서 샘플링된 file_name에 해당하는 모든 데이터 추출\n",
    "print(\"원본 데이터에서 샘플링된 데이터 추출 중...\")\n",
    "result_df = df[df[\"file_name\"].isin(sampled_file_names)]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "print(f\"결과를 '{output_file}'에 저장 중...\")\n",
    "result_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n처리 완료!\")\n",
    "print(f\"총 {len(result_df)}개의 행이 포함된 샘플 데이터가 생성되었습니다.\")\n",
    "print(f\"고유 file_name 개수: {len(sampled_file_names)}개\")\n",
    "print(f\"결과 파일: '{os.path.abspath(output_file)}'\")\n",
    "\n",
    "# 샘플링된 데이터의 category 및 subcategory 분포 확인\n",
    "print(\"\\n샘플링된 데이터의 Category 분포:\")\n",
    "print(result_df[\"category\"].value_counts(normalize=True))\n",
    "print(\"\\n샘플링된 데이터의 Subcategory 분포:\")\n",
    "print(result_df[\"subcategory\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# --- 설정 --- #\n",
    "SAMPLE_SIZE = 2000\n",
    "INPUT_FILENAME = \"normal_dataset2(감정분류).csv\"\n",
    "OUTPUT_FILENAME_PREFIX = \"stratified_sample_emotion\"\n",
    "# --- 설정 끝 --- #\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_file = os.path.join(\"dataset\", INPUT_FILENAME)\n",
    "output_file = os.path.join(\"dataset\", f\"{OUTPUT_FILENAME_PREFIX}_{SAMPLE_SIZE}.csv\")\n",
    "\n",
    "# 데이터 불러오기\n",
    "print(f\"'{input_file}'에서 데이터를 불러오는 중...\")\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(\"데이터 불러오기 완료.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}'을(를) 찾을 수 없습니다. 경로를 확인하세요.\")\n",
    "    exit()\n",
    "\n",
    "# 고유 file_name과 해당 category/subcategory 추출\n",
    "print(\"고유 file_name을 기준으로 데이터 집계 중...\")\n",
    "unique_files = (\n",
    "    df[[\"file_name\", \"category\", \"subcategory\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(f\"총 {len(unique_files)}개의 고유한 file_name을 찾았습니다.\")\n",
    "\n",
    "# 데이터가 요청된 샘플 수보다 적은 경우, 가능한 모든 데이터를 사용\n",
    "if len(unique_files) < SAMPLE_SIZE:\n",
    "    print(\n",
    "        f\"경고: 고유 file_name의 수가 {SAMPLE_SIZE}개({len(unique_files)}개)보다 적습니다.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"가능한 모든 {len(unique_files)}개의 고유 file_name을 사용하여 파일을 생성합니다.\"\n",
    "    )\n",
    "    actual_sample_size = len(unique_files)\n",
    "    output_file = os.path.join(\n",
    "        \"dataset\", f\"{OUTPUT_FILENAME_PREFIX}_{actual_sample_size}.csv\"\n",
    "    )\n",
    "    sampled_files_df = unique_files\n",
    "else:\n",
    "    actual_sample_size = SAMPLE_SIZE\n",
    "    # category와 subcategory를 합쳐서 계층화 키로 사용\n",
    "    unique_files[\"stratify_key\"] = (\n",
    "        unique_files[\"category\"].astype(str)\n",
    "        + \"_\"\n",
    "        + unique_files[\"subcategory\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # 계층적 샘플링 수행\n",
    "    print(f\"{actual_sample_size}개에 대한 계층적 샘플링 수행 중...\")\n",
    "    try:\n",
    "        sampled_files_df, _ = train_test_split(\n",
    "            unique_files,\n",
    "            train_size=actual_sample_size,\n",
    "            stratify=unique_files[\"stratify_key\"],\n",
    "            random_state=42,\n",
    "        )\n",
    "        print(\"샘플링 완료.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"샘플링 오류: {e}\")\n",
    "        print(\n",
    "            \"계층별 최소 샘플 수가 부족할 수 있습니다. stratify 없이 샘플링을 시도합니다.\"\n",
    "        )\n",
    "        sampled_files_df = unique_files.sample(n=actual_sample_size, random_state=42)\n",
    "\n",
    "# 샘플링된 file_name 리스트 추출\n",
    "sampled_file_names = sampled_files_df[\"file_name\"].tolist()\n",
    "\n",
    "# 원본 데이터프레임에서 샘플링된 file_name에 해당하는 모든 데이터 추출\n",
    "print(\"원본 데이터에서 샘플링된 데이터 추출 중...\")\n",
    "result_df = df[df[\"file_name\"].isin(sampled_file_names)]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "print(f\"결과를 '{output_file}'에 저장 중...\")\n",
    "result_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n처리 완료!\")\n",
    "print(f\"총 {len(result_df)}개의 행이 포함된 샘플 데이터가 생성되었습니다.\")\n",
    "print(f\"고유 file_name 개수: {len(sampled_file_names)}개\")\n",
    "print(f\"결과 파일: '{os.path.abspath(output_file)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceec655",
   "metadata": {},
   "source": [
    "# file_name 간략화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608cc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 설정 --- #\n",
    "# 각 파일에 대한 처리 규칙을 정의합니다.\n",
    "# 형식: (파일명, 새 이름 접두사)\n",
    "TASKS = [\n",
    "    (\"stratified_2862_sample.csv\", \"일반통화\"),\n",
    "    (\"stratified_sample_callcenter_6000.csv\", \"콜센터\"),\n",
    "    (\"stratified_sample_emotion_2000.csv\", \"상담대화\"),\n",
    "]\n",
    "\n",
    "DATASET_DIR = \"dataset\"\n",
    "FILENAME_COLUMN = \"file_name\"\n",
    "# --- 설정 끝 --- #\n",
    "\n",
    "\n",
    "def rename_file_names_in_csv(file_path, prefix, column_name):\n",
    "    \"\"\"CSV 파일 내의 지정된 컬럼 값을 새로운 접두사와 숫자로 변경합니다.\"\"\"\n",
    "    print(f\"'{file_path}' 파일 처리 중...\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"-> 파일을 찾을 수 없습니다. 건너뜁니다.\")\n",
    "        return\n",
    "\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"-> '{column_name}' 컬럼을 찾을 수 없습니다. 건너뜁니다.\")\n",
    "        return\n",
    "\n",
    "    # 고유한 기존 file_name 목록 가져오기\n",
    "    unique_names = df[column_name].unique()\n",
    "\n",
    "    # 기존 file_name을 새 이름으로 매핑하는 딕셔너리 생성\n",
    "    # 예: {'original_name_A': '콜센터1', 'original_name_B': '콜센터2', ...}\n",
    "    name_map = {old_name: f\"{prefix}{i+1}\" for i, old_name in enumerate(unique_names)}\n",
    "\n",
    "    # 매핑을 사용하여 컬럼 값 변경\n",
    "    df[column_name] = df[column_name].map(name_map)\n",
    "\n",
    "    # 변경된 데이터프레임을 동일한 파일에 덮어쓰기\n",
    "    df.to_csv(file_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\n",
    "        f\"-> 완료. {len(unique_names)}개의 고유한 file_name이 새로운 형식으로 변경되었습니다.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"file_name 변경 작업을 시작합니다.\")\n",
    "    # 프로젝트 루트에 있는 dataset 폴더를 기준으로 작업\n",
    "    base_path = os.path.join(\".\", DATASET_DIR)\n",
    "\n",
    "    for filename, prefix in TASKS:\n",
    "        file_path = os.path.join(base_path, filename)\n",
    "        rename_file_names_in_csv(file_path, prefix, FILENAME_COLUMN)\n",
    "\n",
    "    print(\"\\n모든 작업이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704b794",
   "metadata": {},
   "source": [
    "# 3개의 데이터셋 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00d50f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "병합할 CSV 파일이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- 설정 --- #\n",
    "# CSV 파일이 있는 디렉토리\n",
    "SOURCE_DIR = \"dataset\"\n",
    "# 병합된 파일을 저장할 이름\n",
    "OUTPUT_FILENAME = \"merged_dataset.csv\"\n",
    "# --- 설정 끝 --- #\n",
    "\n",
    "\n",
    "def merge_all_csv_in_directory(source_dir, output_filename):\n",
    "    \"\"\"지정된 디렉토리의 모든 CSV 파일을 하나로 병합합니다.\"\"\"\n",
    "    # 프로젝트 루트를 기준으로 소스 디렉토리 경로 설정\n",
    "    search_path = os.path.join(\".\", source_dir, \"*.csv\")\n",
    "    output_path = os.path.join(\".\", source_dir, output_filename)\n",
    "\n",
    "    # 소스 디렉토리에서 모든 CSV 파일 목록 가져오기\n",
    "    all_csv_files = glob.glob(search_path)\n",
    "\n",
    "    # 만약 스크립트가 이전에 실행되어 병합된 파일이 존재한다면, 목록에서 제외\n",
    "    if output_path in all_csv_files:\n",
    "        print(f\"기존에 병합된 파일 '{output_path}'을(를) 제외합니다.\")\n",
    "        all_csv_files.remove(output_path)\n",
    "\n",
    "    if not all_csv_files:\n",
    "        print(\"병합할 CSV 파일이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    print(\"다음 파일들을 병합합니다:\")\n",
    "    for f in all_csv_files:\n",
    "        print(f\" - {f}\")\n",
    "\n",
    "    # 모든 CSV 파일을 읽어 데이터프레임 리스트에 추가\n",
    "    dataframe_list = []\n",
    "    for f in all_csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            dataframe_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"'{f}' 파일을 읽는 중 오류 발생: {e}\")\n",
    "\n",
    "    if not dataframe_list:\n",
    "        print(\"데이터를 성공적으로 읽은 파일이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 모든 데이터프레임 병합\n",
    "    print(\"\\n데이터프레임 병합 중...\")\n",
    "    merged_df = pd.concat(dataframe_list, ignore_index=True)\n",
    "\n",
    "    # 결과를 새 CSV 파일로 저장\n",
    "    print(f\"병합된 데이터를 '{output_path}'에 저장 중...\")\n",
    "    merged_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"\\n--- 작업 완료 ---\")\n",
    "    print(f\"총 {len(all_csv_files)}개의 파일이 병합되었습니다.\")\n",
    "    print(\n",
    "        f\"병합된 파일 '{output_path}'에는 총 {len(merged_df)}개의 행이 포함되어 있습니다.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_all_csv_in_directory(SOURCE_DIR, OUTPUT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd73df",
   "metadata": {},
   "source": [
    "# 데이터셋 총 데이터 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa815e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = os.path.join(\"dataset\", \"merged_dataset.csv\")\n",
    "\n",
    "# 데이터 불러오기\n",
    "print(f\"'{file_path}'에서 데이터를 불러오는 중...\")\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    # 고유한 file_name 개수 세기\n",
    "    unique_count = df[\"file_name\"].nunique()\n",
    "    print(\"--- 결과 ---\")\n",
    "    print(f\"고유한 file_name의 개수는 {unique_count}개입니다.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{file_path}'을(를) 찾을 수 없습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4432cb",
   "metadata": {},
   "source": [
    "# 데이터셋에 있는 데이터 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7193f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 설정 --- #\n",
    "INPUT_FILE = \"merged_dataset.csv\"\n",
    "# --- 설정 끝 --- #\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = os.path.join(\"dataset\", INPUT_FILE)\n",
    "\n",
    "# 데이터 불러오기\n",
    "print(f\"'{file_path}'에서 데이터를 불러오는 중...\")\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"데이터 불러오기 완료.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{file_path}'을(를) 찾을 수 없습니다.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"데이터를 읽는 중 오류 발생: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 동일한 file_name의 msg를 하나의 대화로 묶기\n",
    "print(\"동일한 file_name의 msg를 하나의 대화로 묶는 중...\")\n",
    "# 각 file_name별로 msg를 공백으로 연결\n",
    "conversations = (\n",
    "    df.groupby(\"file_name\")[\"msg\"]\n",
    "    .apply(lambda x: \" \".join(x.astype(str)))\n",
    "    .reset_index()\n",
    ")\n",
    "conversations.rename(columns={\"msg\": \"full_conversation\"}, inplace=True)\n",
    "\n",
    "# 대화의 글자 길이 계산\n",
    "print(\"대화의 글자 길이 계산 중...\")\n",
    "conversations[\"conversation_length\"] = conversations[\"full_conversation\"].apply(len)\n",
    "\n",
    "# 글자 길이 통계 확인\n",
    "print(\"\\n--- 대화 글자 길이 통계 --- \")\n",
    "print(conversations[\"conversation_length\"].describe())\n",
    "\n",
    "print(\"\\n분석 완료.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "woogawooga-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
