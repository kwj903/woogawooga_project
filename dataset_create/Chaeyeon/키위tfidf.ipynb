{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "df_phish = pd.read_csv(\"C:/Users/user/Downloads/0708/woogawooga_project/dataset/피싱데이터에서llm테스트뺀것.csv\")\n",
    "df_normal = pd.read_csv(\"C:/Users/user/Downloads/0708/woogawooga_project/dataset/일반통화_남은셋.csv\")\n",
    "\n",
    "# 2. 텍스트 문자열화\n",
    "df_phish[\"text\"] = df_phish[\"text\"].astype(str)\n",
    "df_normal[\"text\"] = df_normal[\"text\"].astype(str)\n",
    "\n",
    "# 3. 보이스피싱 데이터는 file_name 기준으로 대화 단위로 묶기\n",
    "phish_dialogue = df_phish.groupby(\"file_name\")[\"text\"].apply(lambda x: ' '.join(x)).reset_index()\n",
    "phish_dialogue[\"phishing_type\"] = phish_dialogue[\"file_name\"].map(\n",
    "    df_phish.set_index(\"file_name\")[\"phishing_type\"].to_dict()\n",
    ")\n",
    "\n",
    "# 4. Kiwi 형태소 분석기 준비\n",
    "kiwi = Kiwi()\n",
    "\n",
    "#  표제어 기반 토큰화 함수\n",
    "def tokenize_kiwi_lemmatized(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return ' '.join([\n",
    "        token.lemma\n",
    "        for token in tokens\n",
    "        if token.tag.startswith(\"N\") or token.tag.startswith(\"V\") or token.tag.startswith(\"VA\")\n",
    "    ])\n",
    "\n",
    "# 5. 보이스피싱/일반 대화 텍스트를 표제어 기반 토큰화\n",
    "phish_dialogue[\"tokenized_text\"] = phish_dialogue[\"text\"].apply(tokenize_kiwi_lemmatized)\n",
    "normal_text = ' '.join(df_normal[\"text\"])\n",
    "normal_tokenized = tokenize_kiwi_lemmatized(normal_text)\n",
    "\n",
    "# 6. 위험도 계산 (TF-IDF 기반)\n",
    "docs = [' '.join(phish_dialogue[\"tokenized_text\"]), normal_tokenized]\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "phish_scores = tfidf_matrix.toarray()[0]\n",
    "normal_scores = tfidf_matrix.toarray()[1]\n",
    "\n",
    "epsilon = 1e-6\n",
    "risk_scores = phish_scores / (normal_scores + epsilon)\n",
    "risk_dict = dict(zip(words, risk_scores))  # 단어: 위험도 점수\n",
    "\n",
    "# 7. 보이스피싱 유형별 위험 키워드 추출\n",
    "grouped_type = df_phish.groupby(\"phishing_type\")[\"text\"].apply(lambda x: ' '.join(x)).reset_index()\n",
    "grouped_type[\"tokenized_text\"] = grouped_type[\"text\"].apply(tokenize_kiwi_lemmatized)\n",
    "\n",
    "top_keywords_by_type = {}\n",
    "for _, row in grouped_type.iterrows():\n",
    "    ptype = row[\"phishing_type\"]\n",
    "    tokens = row[\"tokenized_text\"].split()\n",
    "\n",
    "    keyword_scores = {word: risk_dict[word] for word in tokens if word in risk_dict}\n",
    "    top_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "    top_keywords_by_type[ptype] = dict(top_keywords)\n",
    "\n",
    "# 8. 결과 저장\n",
    "df_result = pd.DataFrame(top_keywords_by_type).T\n",
    "df_result.to_csv(\"유형별_위험키워드_Kiwi.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"'유형별_위험키워드_Kiwi.csv' 저장 완료 (표제어 기반)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db39eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "woogawooga-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
